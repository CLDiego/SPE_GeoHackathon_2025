{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Workshop Banner](assets/S1_M2.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S1_M2_ChatAgent.ipynb)\n",
    "\n",
    "***\n",
    "# Session 01 // Module 02: LangChain\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Goal: Build a geoscience-focused chat assistant using modern LangChain, and Hugging Face Transformers\n",
    "\n",
    "This module upgrades from basic HF usage to a real chat workflow:\n",
    "- Load a conversational model (Phi-3 / Llama 3.1) efficiently.\n",
    "- Compose prompts and chains using LCEL (LangChain Expression Language).\n",
    "- Add conversation memory with RunnableWithMessageHistory.\n",
    "- Ship an interactive Gradio UI.\n",
    "- Explore an advanced exercise with a larger scientific model.\n",
    "\n",
    "## What we’ll use and why\n",
    "\n",
    "- LangChain (modern API: LCEL, ChatModels, Runnables)\n",
    "  - Compose pipelines with a declarative operator (|).\n",
    "  - Treat “chat” as structured messages, not plain strings.\n",
    "- Hugging Face Transformers\n",
    "  - Load and run open models locally or in Colab.\n",
    "  - Stream tokens, quantize large models with BitsAndBytes.\n",
    "- BitsAndBytes (4-bit quantization)\n",
    "  - Fit larger models in memory; good for Colab GPUs. Not supported on macOS Metal (MPS).\n",
    "- Gradio\n",
    "  - Simple, fast UI to interact with your agent in the browser.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Tip: Prefer running this notebook on Colab with a GPU. On Apple Silicon (MPS), BitsAndBytes 4-bit is not supported — use a small model or CPU-friendly settings (snippet below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment setup\n",
    "!pip -q install langchain langchain-core langchain-community langchain-huggingface torch gradio\n",
    "!pip -q install bitsandbytes==0.46.0 transformers==4.48.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# Retrieving the token is required to get access to HF hub\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model loading and quantization\n",
    "\n",
    "One of the powerful features of Hugging Face Transformers is the ability to run models locally, either on your machine or in a Colab notebook. This gives you more control over the model, data privacy, and cost (no API fees). However, local models can be large and require careful loading to fit in memory.\n",
    "\n",
    "Local LLMs can be large (7B+ parameters) and require careful loading to fit in memory. We’ll use Hugging Face Transformers with BitsAndBytes 4-bit quantization to run a mid-sized model (Phi-3 mini, 6.7B) on a single Colab GPU.\n",
    "\n",
    "4-bit quantization (BitsAndBytes) is a great way to reduce memory usage with minimal quality loss. 8-bit is also an option, but 4-bit is better for larger models (7B+). On macOS (MPS), BitsAndBytes is not supported, so you’ll need to use a small model or CPU-friendly settings.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Hardware notes\n",
    "> - Colab + GPU: 4-bit quantization (BitsAndBytes) is great for 7–8B models.\n",
    "> - macOS (MPS): BitsAndBytes is not supported. Use a small model (≤1–2B), CPU, or skip quantization (see alternative snippet).\n",
    "> - Hugging Face gated models require an HF token and license acceptance.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Key parameters (BitsAndBytesConfig)\n",
    "> - `load_in_4bit=True`: Enable 4-bit weight quantization (huge memory savings).\n",
    "> - `bnb_4bit_quant_type=\"nf4\"`: Quantization scheme tuned for LLMs (good quality). Other options: `fp4` (faster, lower quality), `int4` (legacy).\n",
    "> - `bnb_4bit_compute_dtype=torch.bfloat16`: Arithmetic precision during compute.\n",
    "> - `bnb_4bit_use_double_quant=True`: Second-stage quantization for further memory savings.\n",
    "> - `device_map=\"auto\"`: Automatically place model layers on available devices (GPU/CPU).\n",
    "> - `torch_dtype=torch.float16`: Use half-precision floats to save memory and speed up computation. You can also use `torch.bfloat16` if supported by your hardware, which can offer better performance for some models. The b stands for \"brain float\" and is particularly useful on TPUs and some newer GPUs.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Snippet: Alternative for Mac (skip 4-bit)\n",
    "\n",
    "```python\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # small & MPS-friendly\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.backends.mps.is_available() else torch.float32,\n",
    "    device_map=\"auto\"  # will use 'mps' if available\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "```\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Tip**: If you encounter CUDA out-of-memory errors, try lowering `max_new_tokens`. For higher accuracy, increase quantization bits (use 8-bit instead of 4-bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Use a more conversational model\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Create HuggingFace pipeline\n",
    "# Steps:\n",
    "# 1. Load tokenizer\n",
    "# 2. Create quantization config\n",
    "# 3. Load model with quantization config\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. From Transformers pipeline to a LangChain ChatModel\n",
    "\n",
    "We are going to build a local text-generation stack and expose it through LangChain’s chat interface.\n",
    "\n",
    "- Transformers pipeline\n",
    "  - The Hugging Face `pipeline(\"text-generation\", ...)` wraps your `model` and `tokenizer` and calls `model.generate(...)` under the hood with the decoding parameters you pass.\n",
    "  - It handles tokenization, batched inference, and decoding for you.\n",
    "\n",
    "- LangChain wrappers\n",
    "  - `HuggingFacePipeline`: Bridges a HF pipeline into a LangChain LLM so it can be composed in chains (LCEL).\n",
    "  - `ChatHuggingFace`: Adapts that LLM to a chat-style interface (system/human/ai messages), making it compatible with `ChatPromptTemplate`, `RunnableWithMessageHistory`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chat_models import ChatHuggingFace\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.2,\n",
    "    do_sample=True, # Sampling enables more diverse outputs\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False # The generated text will not include the prompt\n",
    ")\n",
    "\n",
    "# Create LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Wrap with ChatHuggingFace for modern interface\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why these parameters?\n",
    "\n",
    "- `max_new_tokens=150`\n",
    "  - Caps how many tokens the model may generate beyond the prompt. Lower for faster, higher for more complete answers.\n",
    "- `temperature=0.2`\n",
    "  - Controls randomness. 0.2 is conservative and tends to be factual/concise. Increase for more creativity.\n",
    "- `do_sample=True`\n",
    "  - Enables stochastic decoding (required for temperature/top_p/top_k to take effect). Set False for deterministic greedy decoding.\n",
    "- `pad_token_id=tokenizer.eos_token_id`\n",
    "  - Many causal LMs have no pad token. Using EOS avoids padding-related errors in batching.\n",
    "- `return_full_text=False`\n",
    "  - Return only the model’s continuation (not the prompt), which is what you want in most chat UIs.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Tips**:\n",
    "> - Add `top_p=0.9` or `top_k=50` to control sampling. Use one at a time for easier tuning.\n",
    "> - For repetition issues, try `repetition_penalty=1.1` or `no_repeat_ngram_size=2`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompting with ChatPromptTemplate\n",
    "\n",
    "Structure matters for chat models. A good System message defines persona and scope; Human carries the user’s question. For memory-enabled chats, insert MessagesPlaceholder(\"history\").\n",
    "\n",
    "Guidelines for the system prompt:\n",
    "- Define role, expertise, and tone (e.g., Dr. GeoBot).\n",
    "- List do/don’t rules: brevity, technical accuracy, safety.\n",
    "- Encourage step-by-step explanations when math is needed.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Snippet: Quick test\n",
    "\n",
    "```python\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "test = (ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Dr. GeoBot, a concise geoscience expert.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "]) | chat_model | StrOutputParser())\n",
    "\n",
    "test.invoke({\"question\": \"Explain porosity vs permeability.\"})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a system prompt for geoscience expertise\n",
    "# The system prompt sets the behavior and personality of the assistant\n",
    "system_prompt = \"\"\"\n",
    "You are Dr. GeoBot, an expert geophysicist and petroleum engineer with 20 years of experience.\n",
    "You specialize in seismic interpretation, reservoir characterization, and hydrocarbon exploration.\n",
    "\n",
    "Guidelines:\n",
    "- Provide accurate, helpful answers about geoscience topics\n",
    "- Keep responses concise but informative (2-3 sentences)\n",
    "- Use technical terms but explain them when needed\n",
    "- Focus on practical applications and formulas\n",
    "- If unsure, acknowledge limitations\n",
    "\"\"\"\n",
    "\n",
    "# Create chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Test the template\n",
    "test_question = \"What is porosity?\"\n",
    "formatted_prompt = prompt_template.format_messages(question=test_question)\n",
    "print(\"Formatted prompt:\")\n",
    "for message in formatted_prompt:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LCEL chains: from prompt to answer\n",
    "\n",
    "LangChain Expression Language (LCEL) lets you compose pipelines declaratively using the `|` operator. Each component (prompt, model, parser) is a Runnable that takes inputs and produces outputs. The previous wrappers enable us to build a full chat pipeline:\n",
    "\n",
    "- After `llm = HuggingFacePipeline(...)`, you can plug the model into LCEL chains:\n",
    "  - `prompt | chat_model | StrOutputParser()`\n",
    "  - `StrOutputParser()` extracts the text from the model’s response object. It’s useful when you want just the generated text without any additional metadata.\n",
    "- After `chat_model = ChatHuggingFace(llm=llm)`, you can use chat prompts, memory, and multi-turn workflows.\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Quick usage example\n",
    "\n",
    "```python\n",
    "\n",
    "# Compose a minimal chain (later cell)\n",
    "simple_chain = prompt_template | chat_model | StrOutputParser()\n",
    "simple_chain.invoke({\"question\": \"What is seismic inversion?\"})\n",
    "\n",
    "```\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Sanity checks**\n",
    "> - `print(f\"Model loaded: {model_name}\")`: Confirms which repo/model is active.\n",
    "> - `print(f\"Model parameters: {model.num_parameters():,}\")`: Rough size indicator; larger models need more VRAM/quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple chain using LCEL\n",
    "simple_chain = prompt_template | chat_model | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "print(\"=== Testing Simple Chain ===\")\n",
    "response = simple_chain.invoke({\"question\": \"What is the difference between porosity and permeability?\"})\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Low-level Transformers and streaming\n",
    "\n",
    "Sometimes you need raw control (e.g., custom token streaming to console).\n",
    "\n",
    "- `TextStreamer`: Streams decoded tokens as they are generated.\n",
    "- `Manual prompt strings`: Some instruct models respond better with [SYSTEM]/[USER] tags or chat templates (check model card).\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Caveat: Each model has its own chat template. Prefer `tokenizer.apply_chat_template `if available.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Streaming output example\n",
    "\n",
    "```python\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "inputs = tokenizer(\"User: What is porosity?\\nAssistant:\", return_tensors=\"pt\").to(model.device)\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "full_prompt = \"\"\n",
    "for msg in formatted_prompt:\n",
    "    if msg.type == \"system\":\n",
    "        full_prompt += f\"[SYSTEM]\\n{msg.content}\\n\"\n",
    "    elif msg.type == \"human\":\n",
    "        full_prompt += f\"[USER]\\n{msg.content}\\n\"\n",
    "\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Stream tokens as they are generated\n",
    "model.generate(**inputs, streamer=streamer, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple geoscience questions\n",
    "test_questions = [\n",
    "    \"What is seismic resolution?\",\n",
    "    \"How do P-waves differ from S-waves?\",\n",
    "    \"What factors affect hydrocarbon migration?\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Multiple Questions ===\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    response = simple_chain.invoke({\"question\": question})\n",
    "    print(f\"   Answer: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conversation memory with RunnableWithMessageHistory\n",
    "\n",
    "Why memory?\n",
    "- Keep context across turns (references like “this technique”).\n",
    "- Reduces repetition; improves coherence.\n",
    "\n",
    "How it works:\n",
    "- You provide a function that returns a `BaseChatMessageHistory` per `session_id`.\n",
    "- `RunnableWithMessageHistory` injects/updates that history automatically when invoking the chain.\n",
    "\n",
    "Memory best practices:\n",
    "- Keep histories short on small context windows.\n",
    "- Summarize old messages if conversations get long (advanced).\n",
    "- Use session IDs per user/tab.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Memory-enabled chain\n",
    "\n",
    "```python\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory, BaseChatMessageHistory\n",
    "from langchain.chains import RunnableWithMessageHistory\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are Dr. GeoBot...\"),\n",
    "  MessagesPlaceholder(variable_name=\"history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "conv_chain = conversational_prompt | chat_model | StrOutputParser()\n",
    "mem_chain = RunnableWithMessageHistory(\n",
    "  conv_chain,\n",
    "  lambda sid: store.setdefault(sid, ChatMessageHistory()),\n",
    "  input_messages_key=\"question\",\n",
    "  history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory, BaseChatMessageHistory\n",
    "from langchain.chains import RunnableWithMessageHistory\n",
    "\n",
    "# Create conversational prompt template with history\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create the conversational chain\n",
    "conversational_chain = conversational_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# Store for conversation histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create conversational chain with memory\n",
    "conversational_with_memory = RunnableWithMessageHistory(\n",
    "    conversational_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"Conversational chain with memory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 In-depth explanation of memory components\n",
    "\n",
    "The previous cell builds a chat pipeline that remembers prior turns per `session_id`.\n",
    "\n",
    "High-level flow:\n",
    "- Define a chat prompt with a system message + a placeholder for prior messages + the new user question.\n",
    "- Compose the model pipeline with LCEL: prompt → chat model → text output.\n",
    "- Maintain a per-session message history in a simple in-memory store.\n",
    "- Wrap the chain with RunnableWithMessageHistory so history is automatically injected and updated on every call.\n",
    "\n",
    "***\n",
    "\n",
    "Key pieces explained:\n",
    "- `MessagesPlaceholder`\n",
    "  - Acts as a slot in the prompt where past messages (human/ai) will be inserted each turn.\n",
    "  - `variable_name=\"history\"` must match `history_messages_key` in the wrapper.\n",
    "\n",
    "- `ChatMessageHistory`\n",
    "  - An in-memory list of chat messages for one session (human/ai/system).\n",
    "  - Used to persist conversation turns across invocations.\n",
    "  - `BaseChatMessageHistory` is the interface type; `ChatMessageHistory` is the concrete implementation.\n",
    "\n",
    "- `RunnableWithMessageHistory`\n",
    "  - Wraps any LCEL chain (Runnable) and:\n",
    "    - Fetches a session-specific history via your getter (`get_session_history`).\n",
    "    - Injects that history into the prompt at the `MessagesPlaceholder`.\n",
    "    - Appends the new user/ai messages after the run.\n",
    "  - `input_messages_key=\"question\"` tells it which input field is the human message.\n",
    "  - `history_messages_key=\"history\"` tells it which prompt placeholder to fill.\n",
    "\n",
    "***\n",
    "\n",
    "How to call with memory:\n",
    "- Pass a session id via the config:\n",
    "  - `conversational_with_memory.invoke({\"question\": \"...\"}, config={\"configurable\": {\"session_id\": \"my_user\"}})`\n",
    "- Each unique `session_id` gets its own message list in store.\n",
    "- Change `session_id` to start a fresh conversation or clear store to reset.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Tip: `store` is a simple Python dict for demo purposes. For multi-user apps, consider a durable backend (e.g., Redis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversational memory\n",
    "print(\"=== Testing Conversational Memory ===\")\n",
    "\n",
    "session_config = {\"configurable\": {\"session_id\": \"test_session\"}}\n",
    "\n",
    "# First question\n",
    "response1 = conversational_with_memory.invoke(\n",
    "    {\"question\": \"What is seismic inversion?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q1: What is seismic inversion?\")\n",
    "print(f\"A1: {response1}\")\n",
    "print()\n",
    "\n",
    "# Follow-up question that refers to previous context\n",
    "response2 = conversational_with_memory.invoke(\n",
    "    {\"question\": \"What are the main types of this technique?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q2: What are the main types of this technique?\")\n",
    "print(f\"A2: {response2}\")\n",
    "print()\n",
    "\n",
    "# Another follow-up\n",
    "response3 = conversational_with_memory.invoke(\n",
    "    {\"question\": \"Which type is most commonly used in the industry?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q3: Which type is most commonly used in the industry?\")\n",
    "print(f\"A3: {response3}\")\n",
    "print()\n",
    "\n",
    "# Check memory content\n",
    "print(\"=== Current Memory ===\")\n",
    "history = get_session_history(\"test_session\")\n",
    "for message in history.messages:\n",
    "    print(f\"{message.type}: {message.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting and performance\n",
    "\n",
    "- CUDA out of memory\n",
    "  - Lower max_new_tokens, increase quantization, choose a smaller model.\n",
    "- Slow generation\n",
    "  - Reduce max_new_tokens, temperature; use smaller models; close other notebooks.\n",
    "- BitsAndBytes not available (macOS)\n",
    "  - Use CPU/MPS without quantization; pick a 1–2B model; or run on Colab GPU.\n",
    "- Tokenizer/pad errors\n",
    "  - Ensure tokenizer.pad_token is set (fallback to eos_token).\n",
    "- HF token issues\n",
    "  - In Colab: from google.colab import userdata; userdata.get(\"HF_TOKEN\")\n",
    "  - Locally: export HF_TOKEN=... (macOS: add to ~/.zshrc), then use use_auth_token in from_pretrained if needed.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> macOS: set token for terminal sessions\n",
    "\n",
    "```bash\n",
    "# macOS (zsh)\n",
    "echo 'export HF_TOKEN=YOUR_TOKEN' >> ~/.zshrc\n",
    "source ~/.zshrc\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
