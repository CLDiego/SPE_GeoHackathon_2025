{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "![Workshop Banner](assets/S2_M2.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S2_M2_Advanced_agents.ipynb)\n",
    "\n",
    "***\n",
    "\n",
    "# Session 02 // Module 02: Advanced Agents for Geoscience (RAG + SQL Analyst)\n",
    "\n",
    "In this module you’ll build an agentic system that can both:\n",
    "- Retrieve domain knowledge from JSON + PDFs (RAG with citations)\n",
    "- Analyze well production data from a CSV using SQL (DuckDB)\n",
    "\n",
    "You’ll learn how LangChain Agents differ from fixed chains, how to create tools, and how to compose them into a single agent that chooses the right tool at runtime (ReAct).\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the think → act → observe loop for Agents\n",
    "- Build a RAG tool that returns concise, cited answers\n",
    "- Build a SQL analyst tool that computes stats over tabular data\n",
    "- Combine tools into a master ReAct-style agent with routing\n",
    "- Ship a simple Gradio chat UI\n",
    "\n",
    "## What you’ll build\n",
    "- A local vector DB (Chroma) indexing JSON + PDFs\n",
    "- A SQL-powered CSV analyst (DuckDB) wrapped as a tool\n",
    "- A ReAct agent that selects between tools (and can use both)\n",
    "- An interactive UI for unified querying\n",
    "\n",
    "## What we’ll use and why\n",
    "- LangChain (agents, tools, LCEL)\n",
    "  - Tools encapsulate capabilities; ReAct agents plan and call tools\n",
    "- Hugging Face Transformers\n",
    "  - Local LLM for generation (Phi-3 mini or similar)\n",
    "- Chroma + sentence-transformers\n",
    "  - Fast local embeddings + vector store for RAG\n",
    "- DuckDB (in-memory SQL over DataFrames/CSV)\n",
    "  - Fast analytics with familiar SQL, safe by constraining to SELECT-only\n",
    "- Gradio\n",
    "  - Lightweight, shareable UI\n",
    "\n",
    "> Tip: Prefer running on Colab with a GPU. On Apple Silicon (MPS), 4-bit quantization (bitsandbytes) isn’t supported—use float16 and smaller models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Introduction to Agents\n",
    "\n",
    "Agents follow a loop:\n",
    "- Think: reason about the question and plan\n",
    "- Act: call a tool and pass it arguments\n",
    "- Observe: read tool output, refine plan, repeat until done\n",
    "\n",
    "How Agents differ from Chains:\n",
    "- Chains are fixed graphs (e.g., retrieve → prompt → generate). Great for predictable, stable flows.\n",
    "- Agents are dynamic planners. They choose tools, order, and parameters at runtime based on the query.\n",
    "\n",
    "When to use which:\n",
    "- Use a Chain when the path is known and stable (RAG).\n",
    "- Use an Agent when users may ask diverse questions requiring different tools (RAG vs CSV) or multiple steps.\n",
    "\n",
    "Safety notes:\n",
    "- Tools can do arbitrary actions. Keep tool descriptions precise.\n",
    "- If you allow code execution (Pandas agent), prefer sandboxed environments and read-only data for demos.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Snippet: Minimal Agent thought loop (conceptual)\n",
    "```text\n",
    "Question -> [Agent thinks]\n",
    "Action: geoscience_retriever\n",
    "Observation: \"...cited context...\"\n",
    "[Agent thinks]\n",
    "Action: csv_analyst\n",
    "Observation: \"...average water cut = 23.1%...\"\n",
    "Final Answer: \"...concise synthesis + sources\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/SPE_GeoHackathon_2025/refs/heads/dev/spe_utils.txt -O spe_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=5 -i spe_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core LangChain + tooling\n",
    "!pip -q install langchain langchain-core langchain-community langchain-huggingface langchain-text-splitters langchain-chroma\n",
    "!pip -q install langchain-experimental gradio pandas duckdb\n",
    "\n",
    "# HF + models + vecstore deps\n",
    "!pip -q install transformers accelerate bitsandbytes huggingface_hub chromadb pypdf\n",
    "\n",
    "# Web rendering for PDF scraping\n",
    "!pip -q install playwright nest_asyncio\n",
    "!python -m playwright install chromium\n",
    "\n",
    "# Optional telemetry fixes in some envs\n",
    "!pip -q install --upgrade opentelemetry-api opentelemetry-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Hugging Face token (optional). On Colab, you can use google.colab.userdata; otherwise env var.\n",
    "import os\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN', None)\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN') or HF_TOKEN\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from huggingface_hub import login\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        login(token=HF_TOKEN)\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"HF token set:\" , bool(HF_TOKEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Settings and Paths\n",
    "\n",
    "We set paths for data and a persistent vector DB to speed up re-runs.\n",
    "\n",
    "- WORKDIR/DATA_DIR/PDF_DIR: local folders for JSON/PDFs and outputs\n",
    "- DB_DIR: persisted Chroma database\n",
    "- MODEL_EMBED: fast sentence embedding model\n",
    "- LLM_NAME: local LLM for generation (Phi-3 mini is a good balance)\n",
    "- CHUNK_SIZE/CHUNK_OVERLAP: split documents for better retrieval\n",
    "- TOP_K: number of chunks to retrieve per question\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Hardware notes\n",
    "- CUDA (Colab GPU): can use 4-bit quantization for 7–8B models.\n",
    "- Apple MPS: use float16 (no 4-bit). Consider ≤2B models if memory is tight.\n",
    "- CPU: use float32 and small models; expect slower generation.\n",
    "\n",
    "> Key parameters\n",
    "- CHUNK_SIZE/OVERLAP: start with 1000/200; adjust based on corpus density\n",
    "- TOP_K: 4–8 is typical; reduce to limit noise, increase to avoid missing facts\n",
    "- MODEL_EMBED: All-MiniLM-L6-v2 is fast and accurate for semantic search\n",
    "\n",
    "> Token handling\n",
    "- Set tokenizer.pad_token to eos_token if missing to avoid padding errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKDIR = Path.cwd()\n",
    "DATA_DIR = WORKDIR / \"raw_data\"\n",
    "PDF_DIR = DATA_DIR / \"pdfs\"\n",
    "DB_DIR = WORKDIR / \"local_data\" / \"geo_vector_db_agents\"\n",
    "DB_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Models\n",
    "MODEL_EMBED = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Chunking + Retrieval\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K = 6\n",
    "\n",
    "# Dataset source (JSON) used in S2_M1\n",
    "HF_DATASET = \"GainEnergy/ogdataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Tool 1: Geoscience RAG Retriever\n",
    "\n",
    "We refactor the RAG pipeline into functions and include both JSON and PDFs.\n",
    "\n",
    "Pipeline steps:\n",
    "1) Download/ensure JSON dataset locally (HF dataset snapshot)\n",
    "2) Load JSON and PDFs into LangChain Documents with consistent metadata\n",
    "3) Split into chunks (RecursiveCharacterTextSplitter)\n",
    "4) Embed chunks and persist to Chroma\n",
    "5) Build a retrieval chain with a concise, grounded prompt\n",
    "6) Wrap it as a LangChain Tool returning an answer plus brief sources\n",
    "\n",
    "> Key parameters (Chroma + Embeddings)\n",
    "- model_name: sentence-transformers/all-MiniLM-L6-v2\n",
    "- chunk_size/chunk_overlap: 1000/200\n",
    "- persist_directory: speed up dev re-runs\n",
    "\n",
    "> Prompt guidance (system)\n",
    "- Be helpful and concise\n",
    "- Use provided context; say “don’t know” if unclear\n",
    "- Include brief source titles in the answer\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Snippet: Consistent metadata for better citations\n",
    "```python\n",
    "# Ensure PDFs yield titles and file paths\n",
    "for d in pages:\n",
    "    d.metadata = {\n",
    "        **(d.metadata or {}),\n",
    "        \"title\": d.metadata.get(\"title\") or p.stem,\n",
    "        \"file_path\": str(p)\n",
    "    }\n",
    "```\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Retrieval tips\n",
    "- If answers are vague, preview retrieved chunks and reduce TOP_K\n",
    "- If missing details, increase TOP_K or reduce chunk size for finer granularity\n",
    "- Ensure titles/topics are present for readable citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "from spe_utils import ensure_json_dataset, load_json_docs, load_pdf_docs\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "def build_vectorstore(docs: List[Document], persist_dir: Path, recreate: bool = True) -> Chroma:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"Split {len(docs)} docs into {len(chunks)} chunks.\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=MODEL_EMBED)\n",
    "    if recreate and persist_dir.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(persist_dir)\n",
    "    vs = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=str(persist_dir))\n",
    "    print(f\"Vector DB at {persist_dir}\")\n",
    "    return vs\n",
    "\n",
    "\n",
    "def prepare_rag_corpus(data_dir: Path, pdf_dir: Path, db_dir: Path) -> Chroma:\n",
    "    json_path = ensure_json_dataset(data_dir, HF_DATASET)\n",
    "    print(f\"Using JSON corpus: {json_path}\")\n",
    "    jdocs = load_json_docs(json_path)\n",
    "    pdocs = load_pdf_docs(pdf_dir)\n",
    "    print(f\"Loaded {len(jdocs)} JSON docs; {len(pdocs)} PDF pages.\")\n",
    "    all_docs = jdocs + pdocs\n",
    "    if not all_docs:\n",
    "        raise RuntimeError(\"No documents found. Add PDFs to raw_data/pdfs or check JSON dataset.\")\n",
    "    return build_vectorstore(all_docs, db_dir, recreate=True)\n",
    "\n",
    "\n",
    "vectorstore = prepare_rag_corpus(DATA_DIR, PDF_DIR, DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a3efe",
   "metadata": {},
   "source": [
    "## 7. RAG Chain Assembly\n",
    "\n",
    "We compose:\n",
    "- retriever = vectorstore.as_retriever(k=TOP_K)\n",
    "- prompt = ChatPromptTemplate(system + human with {context} and {input})\n",
    "- document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "- rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "Output shape:\n",
    "- rag_chain returns a dict with keys: {\"answer\", \"context\"}\n",
    "  - answer: model output grounded in context\n",
    "  - context: retrieved Document list for citations\n",
    "\n",
    "Tip:\n",
    "- Adjust TOP_K and chunking if answers feel noisy or incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "def build_generation_pipeline(model_id: str):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    use_mps = getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model_kwargs = {\"device_map\": \"auto\"}\n",
    "\n",
    "    if use_cuda:\n",
    "        try:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quant = BitsAndBytesConfig(load_in_4bit=True)\n",
    "            model_kwargs[\"quantization_config\"] = quant\n",
    "            print(\"CUDA: 4-bit quantization enabled.\")\n",
    "        except Exception:\n",
    "            model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "            print(\"CUDA: fallback to float16.\")\n",
    "    elif use_mps:\n",
    "        model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "        print(\"MPS: using float16 (bitsandbytes 4-bit not supported on MPS).\")\n",
    "    else:\n",
    "        model_kwargs[\"torch_dtype\"] = torch.float32\n",
    "        print(\"CPU: using float32 (slow).\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.2,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
    "    chat_model = ChatHuggingFace(llm=llm)\n",
    "    return llm, chat_model\n",
    "\n",
    "llm, chat_model = build_generation_pipeline(LLM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful geoscience assistant. Use the provided context to answer concisely. If unsure, say you don't know. Include brief source titles.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {input}\")\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, rag_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "print(\"RAG chain ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "@tool(\"geoscience_retriever\")\n",
    "def geoscience_retriever(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool for conceptual/qualitative geoscience questions. It queries an indexed corpus (JSON + PDFs)\n",
    "    and returns a concise answer with brief citations. Input should be a standalone question.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = rag_chain.invoke({\"input\": query})\n",
    "    except Exception as e:\n",
    "        return f\"RAG error: {e}\"\n",
    "    answer = result.get(\"answer\", \"\")\n",
    "    ctx: List[Document] = result.get(\"context\", [])\n",
    "    cites = []\n",
    "    seen = set()\n",
    "    for d in ctx:\n",
    "        md = d.metadata or {}\n",
    "        title = md.get(\"title\") or md.get(\"topic\") or md.get(\"source\") or md.get(\"file_path\") or \"Untitled\"\n",
    "        if title in seen:\n",
    "            continue\n",
    "        seen.add(title)\n",
    "        cites.append(f\"- {title}\")\n",
    "    if cites:\n",
    "        answer = f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(cites)\n",
    "    return answer or \"No answer generated. Try rephrasing.\"\n",
    "\n",
    "print(\"RAG tool 'geoscience_retriever' ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 8. Tool 2: SQL Data Analyst (DuckDB over CSV)\n",
    "\n",
    "We analyze well_production.csv using SQL. The tool generates a single SELECT query (via the chat model), enforces safety constraints, executes it in DuckDB, and returns both the SQL and a compact result table.\n",
    "\n",
    "Dataset schema (per row):\n",
    "- date: YYYY-MM-DD (string)\n",
    "- well_name: string\n",
    "- oil_rate_bpd, water_rate_bpd, gas_rate_scfd: numeric\n",
    "- water_cut_pct: numeric (pre-computed in the sample)\n",
    "\n",
    "What the code does:\n",
    "- ensure_sample_csv: creates a synthetic CSV (3 wells × 60 days) if missing\n",
    "- duckdb.connect + con.register(\"well_prod\", df): exposes the DataFrame as SQL table\n",
    "- _schema_hint: produces a readable schema description for prompting\n",
    "- SQL_GUIDE: system prompt with strict constraints (SELECT-only, table+columns only)\n",
    "- _extract_sql: extracts and validates a single SELECT; blocks DDL/DML and multi-statements\n",
    "- Fuzzy helpers: light auto-correction for column casing and near-miss well names\n",
    "- csv_sql_analyst tool: formats messages, invokes the chat model to get SQL, executes in DuckDB, returns SQL + result\n",
    "\n",
    "Safety constraints:\n",
    "- Only SELECT queries are allowed (INSERT/UPDATE/DELETE/DDL blocked)\n",
    "- Single statement only; early split on \";\" and validation\n",
    "- Limited output (head(20)) to keep chat concise\n",
    "\n",
    "Prompting tips:\n",
    "- Mention operations explicitly: “average,” “sum,” “by well,” “between dates”\n",
    "- Dates are strings; cast if needed: CAST(date AS DATE)\n",
    "- Column names are case sensitive; the tool attempts case correction but keep names exact\n",
    "\n",
    "Examples:\n",
    "- Average water cut for a well:\n",
    "  SELECT AVG(water_cut_pct) FROM well_prod WHERE well_name = 'Well-15/9-F-1-C';\n",
    "- Total oil across all wells:\n",
    "  SELECT SUM(oil_rate_bpd) FROM well_prod;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from spe_utils import ensure_sample_csv\n",
    "\n",
    "CSV_PATH = DATA_DIR / \"well_production.csv\"\n",
    "\n",
    "ensure_sample_csv(CSV_PATH)\n",
    "well_prod = pd.read_csv(CSV_PATH)\n",
    "print(well_prod.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d323f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "well_prod[well_prod.well_name == 'Well-15/9-F-11-A'].water_cut_pct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "\n",
    "from spe_utils.agents import _find_closest_match, _extract_sql\n",
    "\n",
    "# Register the DataFrame as a DuckDB table\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "con.register(\"well_prod\", well_prod)\n",
    "\n",
    "\n",
    "def _schema_hint(df: pd.DataFrame, table_name: str) -> str:\n",
    "    desc = {\n",
    "        \"date\": \"Date string in YYYY-MM-DD\",\n",
    "        \"well_name\": \"Well name\",\n",
    "        \"oil_rate_bpd\": \"Oil rate (barrels per day)\",\n",
    "        \"water_rate_bpd\": \"Water rate (barrels per day)\",\n",
    "        \"gas_rate_scfd\": \"Gas rate (scf per day)\",\n",
    "        \"water_cut_pct\": \"Water cut (%)\",\n",
    "    }\n",
    "    lines = []\n",
    "    for c, t in df.dtypes.items():\n",
    "        lines.append(f\"- {c} ({str(t)}): {desc.get(c, '')}\".rstrip(\": \"))\n",
    "    return f\"Table Name: {table_name}\\nSchema:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "SQL_GUIDE = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You write a single DuckDB SQL SELECT query for the table provided.\\n\"\n",
    "     \"Constraints:\\n\"\n",
    "     \"- Use only the table and its columns as provided in the schema.\\n\"\n",
    "     \"- Use ONLY the provided table identifier.\\n\"\n",
    "     \"- Only return ONE SELECT statement. No CTEs with multiple statements, no DDL/DML.\\n\"\n",
    "     \"- Dates are strings 'YYYY-MM-DD'. If needed, use CAST(date AS DATE).\\n\"\n",
    "     \"- Prefer standard SQL: WHERE, GROUP BY, ORDER BY, LIMIT, AVG, SUM, MIN, MAX, COUNT.\\n\"\n",
    "     \"- Column names are case sensitive as given. If the user uses a column name that is not in the schema, find the closest matching column name from the schema and use that instead.\\n\"\n",
    "     \"- For string values in WHERE clauses (like well names), if the user's value is not an exact match to existing values, try to find the closest existing value in that column and use that. Provide a note if a correction was made.\\n\"\n",
    "     \"{schema}\\n\"\n",
    "     \"Example 1 (average water cut for a well): Given a table named 'well_prod' with a 'well_name' column, and the question 'average water cut for Well-15/9-F-1-C'\\n\"\n",
    "     \"SELECT AVG(water_cut_pct) FROM well_prod WHERE well_name = 'Well-15/9-F-1-C';\\n\"\n",
    "     \"Example 2 (total oil rate across all wells): Given a table named 'well_prod' with an 'oil_rate_bpd' column, and the question 'total oil rate'\\n\"\n",
    "     \"SELECT SUM(oil_rate_bpd) FROM well_prod;\"\n",
    "    ),\n",
    "    (\"human\", \"Question: {question}\\nReturn only the SQL.\")\n",
    "])\n",
    "\n",
    "\n",
    "@tool(\"csv_sql_analyst\")\n",
    "def csv_sql_analyst(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool for quantitative questions over well_production via SQL (DuckDB).\n",
    "    It dynamically generates a single SELECT query and executes it safely.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        msgs = SQL_GUIDE.format_messages(schema=_schema_hint(well_prod, \"well_prod\"), question=question)\n",
    "        resp = chat_model.invoke(msgs)\n",
    "        sql = _extract_sql(\n",
    "            getattr(resp, \"content\", str(resp)),\n",
    "            table_columns=list(well_prod.columns),\n",
    "            string_value_domains={\"well_name\": well_prod[\"well_name\"].unique().tolist()},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"SQL generation error: {e}\"\n",
    "\n",
    "    try:\n",
    "        df_res = con.execute(sql).df()\n",
    "        if df_res.empty:\n",
    "            tbl = \"(no rows)\"\n",
    "        else:\n",
    "            tbl = df_res.head(20).to_string(index=False)\n",
    "        return f\"SQL:\\n{sql}\\n\\nResult:\\n{tbl}\"\n",
    "    except Exception as e:\n",
    "        return f\"SQL execution error: {e}\"\n",
    "\n",
    "\n",
    "print(\"CSV tool 'csv_sql_analyst' ready (DuckDB).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8d0f4",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 5. Master Agent (Tool Routing)\n",
    "\n",
    "Now we assemble the master agent. This agent's primary job is not to answer questions directly, but to **route the user's query to the correct tool**. It has access to both the `geoscience_retriever` (for conceptual questions) and the `csv_analyst` (for data questions).\n",
    "\n",
    "### 5.1. Agent Initialization\n",
    "\n",
    "We use `initialize_agent` to create our master agent. The key is to provide it with the list of available tools and a carefully crafted system prompt.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **The System Prompt is Critical**\n",
    "> The `system_prompt` acts as the agent's instruction manual. It must clearly and concisely describe what each tool does. The agent will read this prompt and the docstrings of each tool to decide which one to use. A well-written prompt is the most important factor for successful tool routing.\n",
    "\n",
    "### 5.2. Agent Type\n",
    "\n",
    "We use `AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`, which is a powerful agent type designed for chat models and capable of handling tools with multiple inputs. It follows the ReAct (Reason+Act) framework, which makes its decision-making process transparent. When you set `verbose=True`, you will see the agent's \"chain of thought\" as it reasons about which tool to use, executes it, and observes the outcome.\n",
    "\n",
    "## 5. Master Agent (tool routing)\n",
    "\n",
    "We combine both tools and let a ReAct agent pick the right one at runtime.\n",
    "\n",
    "- Tools: geoscience_retriever (RAG) and csv_analyst (Pandas)\n",
    "- Model: chat_model (chat wrapper over HF pipeline)\n",
    "- Agent type: STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    "  - Produces a structured thought + tool call plan\n",
    "  - More robust formatting than plain Zero-Shot\n",
    "\n",
    "System message guidance:\n",
    "- Route conceptual/qualitative questions to geoscience_retriever\n",
    "- Route numeric/CSV questions to csv_analyst\n",
    "- If both apply, run them in sequence and synthesize the final answer\n",
    "- Keep answers concise; include sources for RAG\n",
    "\n",
    "> Tool selection heuristics\n",
    "- If the question mentions “average/mean/sum/min/max”, “filter”, or “by well/date”, prefer csv_analyst\n",
    "- If the question asks “explain/define/describe”, prefer geoscience_retriever\n",
    "- For combined tasks (e.g., “Summarize geology and total oil”), call both\n",
    "\n",
    "> Debugging\n",
    "- Set verbose=True to see the ReAct trace (Thought/Action/Observation)\n",
    "- If the agent hallucinates tool names, tighten tool descriptions and system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [geoscience_retriever, csv_sql_analyst]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a geoscience assistant.\\n\"\n",
    "    \"- Use 'geoscience_retriever' for conceptual or qualitative questions about geology, geophysics, drilling, reservoir engineering, etc., grounded in the indexed corpus (JSON + PDFs).\\n\"\n",
    "    \"- Use 'csv_sql_analyst' for questions that require computing statistics or filtering rows from the well_production.csv dataset via SQL.\\n\"\n",
    "    \"If both are relevant, use them in sequence and combine results. Be concise and include brief sources for RAG answers.\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=chat_model,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    agent_kwargs={\"system_message\": system_prompt},\n",
    ")\n",
    "print(\"Master agent ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 6. Testing the Agent\n",
    "\n",
    "Try three scenarios:\n",
    "- RAG: “Explain the concept of seismic resolution.”\n",
    "- CSV: “What is the average water cut for Well-15/9-F-1 C?”\n",
    "- Combined: “Summarize the geology of the Volve field and tell me the total oil production.”\n",
    "\n",
    "Interpreting output:\n",
    "- With verbose=True, you’ll see the agent’s reasoning steps and tool calls\n",
    "- RAG answers should end with brief Sources (titles)\n",
    "- CSV answers should include the numeric result and minimal explanation\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Tips\n",
    "- If the agent times out, reduce max_iterations for csv agent and shorten max_new_tokens for the LLM\n",
    "- If tool routing seems off, make tool descriptions more explicit and add examples in the system message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"how many rows in the well_production.csv dataset has?\",\n",
    "    \"What is the average water cut pct for Well-15/9-F-11-A in the dataframe?\",\n",
    "    \"Summarize the geology of the Volve field and tell me the total oil production.\",\n",
    "]\n",
    "\n",
    "# Optional: scrape a web page to PDF to ensure at least one PDF exists\n",
    "try:\n",
    "    from spe_utils import webpage_to_pdf\n",
    "    sample_url = \"https://en.wikipedia.org/wiki/Seismic_resolution\"\n",
    "    sample_pdf = PDF_DIR / \"seismic_resolution_sample.pdf\"\n",
    "    if not sample_pdf.exists():\n",
    "        print(\"Scraping sample web page to PDF ...\")\n",
    "        webpage_to_pdf(sample_url, sample_pdf)\n",
    "        print(f\"Saved: {sample_pdf}\")\n",
    "        # Rebuild vectorstore to index the new PDF\n",
    "        vectorstore = prepare_rag_corpus(DATA_DIR, PDF_DIR, DB_DIR)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "        from langchain.chains import create_retrieval_chain\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful geoscience assistant. Use the provided context to answer concisely. If unsure, say you don't know. Include brief source titles.\"),\n",
    "            (\"human\", \"Context:\\n{context}\\n\\nQuestion: {input}\")\n",
    "        ])\n",
    "        document_chain = create_stuff_documents_chain(llm, rag_prompt)\n",
    "        rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "except Exception as e:\n",
    "    print(\"Web-to-PDF step skipped:\", e)\n",
    "\n",
    "for q in tests:\n",
    "    print(\"\\n=== Q:\", q)\n",
    "    try:\n",
    "        ans = agent.invoke(q)\n",
    "        print(ans)\n",
    "    except Exception as e:\n",
    "        print(\"Agent error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 7. Gradio UI Integration\n",
    "\n",
    "We wrap the master agent in a small chat UI.\n",
    "\n",
    "UI pieces:\n",
    "- respond(message, history): calls agent.run and appends reply\n",
    "- Chatbot: keeps the conversation visible\n",
    "- Textbox + Buttons: send and clear\n",
    "- demo.launch(share=False): local serving\n",
    "\n",
    "> Deployment notes\n",
    "- In some environments you may need: demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
    "- share=True gets a temporary public URL (avoid for sensitive data)\n",
    "\n",
    "> UX tips\n",
    "- Keep answers concise; show citations for RAG in the message or a side panel\n",
    "- For longer analyses, stream partial results or show “running…” status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def respond(message: str, history: List[Tuple[str, str]]):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    try:\n",
    "        reply = agent.run(message)\n",
    "    except Exception as e:\n",
    "        reply = f\"Agent error: {e}\"\n",
    "    history = history + [(message, reply)]\n",
    "    return \"\", history\n",
    "\n",
    "\n",
    "def ingest_pdf(file_obj) -> str:\n",
    "    try:\n",
    "        import shutil\n",
    "        from spe_utils import load_pdf_docs\n",
    "        # Save uploaded PDF into PDF_DIR\n",
    "        dest = PDF_DIR / Path(file_obj.name).name\n",
    "        shutil.copy(file_obj.name, dest)\n",
    "        # Load and add to vectorstore\n",
    "        docs = load_pdf_docs(PDF_DIR)\n",
    "        # Rebuild vectorstore minimally: append mode not shown in quick demo, rebuild instead\n",
    "        global vectorstore, retriever, rag_chain\n",
    "        vectorstore = prepare_rag_corpus(DATA_DIR, PDF_DIR, DB_DIR)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        print(\"Vectorstore updated after PDF upload.\")\n",
    "        return f\"Uploaded and indexed: {dest.name}\"\n",
    "    except Exception as e:\n",
    "        return f\"PDF ingest error: {e}\"\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"Geo Agent: RAG + CSV Analyst\") as demo:\n",
    "    gr.Markdown(\"## Geo Agent: RAG + CSV Analyst\\nAsk conceptual questions or data questions about well_production.csv.\")\n",
    "    chatbot = gr.Chatbot(height=420)\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"Ask about seismic resolution or well production stats...\", scale=4)\n",
    "        send = gr.Button(\"Send\", variant=\"primary\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    with gr.Row():\n",
    "        uploader = gr.File(label=\"Upload PDF to index\", file_types=[\".pdf\"], interactive=True)\n",
    "        upload_btn = gr.Button(\"Ingest PDF\")\n",
    "\n",
    "    send.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: [], outputs=chatbot)\n",
    "    upload_btn.click(lambda f: ingest_pdf(f), inputs=[uploader], outputs=[])\n",
    "\n",
    "try:\n",
    "    demo.launch(share=False)\n",
    "except Exception as e:\n",
    "    print(\"Gradio failed to launch:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6844863",
   "metadata": {},
   "source": [
    "## 12. Troubleshooting and Tips\n",
    "\n",
    "RAG quality\n",
    "- Empty/weak answers: preview retrieved chunks; tune TOP_K; reduce CHUNK_SIZE or increase OVERLAP\n",
    "- Irrelevant context: lower TOP_K or adjust splitter to improve chunk boundaries\n",
    "- Missing titles in citations: verify metadata fields (“title”, “topic”, “file_path”)\n",
    "\n",
    "Generation performance\n",
    "- CUDA OOM: lower max_new_tokens; use 4-bit on CUDA; switch to a smaller LLM\n",
    "- MPS (macOS): no bitsandbytes 4-bit; use float16; consider ≤2B models\n",
    "- CPU: float32 is slow—minimize tokens and test with tiny models\n",
    "\n",
    "SQL tool\n",
    "- Parsing errors: keep questions specific; include columns/filters explicitly\n",
    "- Case sensitivity: column names are case sensitive; prefer exact names\n",
    "- Dates: stored as strings; cast when needed (CAST(date AS DATE))\n",
    "- No rows returned: check well_name spelling; the tool applies fuzzy correction but verify input\n",
    "- Security: SELECT-only guard blocks multi-statements and DDL/DML\n",
    "\n",
    "Agent routing\n",
    "- Strengthen tool descriptions and system guidance\n",
    "- Provide examples of when to use each tool\n",
    "- If the agent mixes Final Answer with Action, remind it to separate steps (ReAct discipline)\n",
    "\n",
    "HF token and access\n",
    "- Colab: from google.colab import userdata; userdata.get(\"HF_TOKEN\")\n",
    "- macOS zsh: echo 'export HF_TOKEN=YOUR_TOKEN' >> ~/.zshrc && source ~/.zshrc\n",
    "\n",
    "Quick knobs to tweak\n",
    "```python\n",
    "# Retrieval\n",
    "TOP_K = 4  # try 4–8\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 160\n",
    "\n",
    "# Generation\n",
    "temperature = 0.2\n",
    "max_new_tokens = 192\n",
    "\n",
    "# SQL display\n",
    "MAX_ROWS = 20  # head limit for chat output\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
