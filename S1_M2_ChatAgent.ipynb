{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S1_M2_ChatAgent.ipynb)\n",
    "\n",
    "***\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Follow along by running each cell in order\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Make sure to run the environment setup cells first\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Wait for each installation to complete before proceeding\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if installations take a while - this is normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment setup\n",
    "!pip -q install langchain langchain-core langchain-community langchain-huggingface torch gradio\n",
    "!pip -q install bitsandbytes==0.46.0 transformers==4.48.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# Retrieving the token is required to get access to HF hub\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 01 // Module 02: First Chat Agent with LangChain\n",
    "\n",
    "In this module, we'll build our first conversational AI agent using modern LangChain. We'll create a geoscience-focused chatbot that can answer questions about geology, geophysics, and petroleum engineering concepts.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand modern LangChain fundamentals (LCEL, ChatModels, Messages)\n",
    "- Build a simple Q&A chat agent with Hugging Face models\n",
    "- Add conversational memory to maintain context\n",
    "- Create an interactive Gradio interface\n",
    "- Apply the agent to geoscience conversations\n",
    "\n",
    "## 1. Modern LangChain Basics\n",
    "\n",
    "**LangChain** has evolved significantly. Modern LangChain uses:\n",
    "- **LCEL (LangChain Expression Language)**: Declarative way to compose chains\n",
    "- **ChatModels**: Specialized for conversational AI\n",
    "- **Messages**: Structured conversation format\n",
    "- **Runnables**: Standardized interface for all components\n",
    "- **Memory**: More flexible conversation state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting up the Modern Language Model\n",
    "\n",
    "Let's use a modern approach with ChatModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a more conversational model\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"facebook/galactica-1.3b\"\n",
    "\n",
    "# Create HuggingFace pipeline\n",
    "# Steps:\n",
    "# 1. Load tokenizer\n",
    "# 2. Create quantization config\n",
    "# 3. Create prompt model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config)\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.2,\n",
    "    do_sample=True, # Sampling enables more diverse outputs\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False # The generated text will not include the prompt\n",
    ")\n",
    "\n",
    "# Create LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Wrap with ChatHuggingFace for modern interface\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating Modern Prompt Templates\n",
    "\n",
    "Modern LangChain uses ChatPromptTemplate with structured messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a system prompt for geoscience expertise\n",
    "# The system prompt sets the behavior and personality of the assistant\n",
    "system_prompt = \"\"\"\n",
    "You are Dr. GeoBot, an expert geophysicist and petroleum engineer with 20 years of experience.\n",
    "You specialize in seismic interpretation, reservoir characterization, and hydrocarbon exploration.\n",
    "\n",
    "Guidelines:\n",
    "- Provide accurate, helpful answers about geoscience topics\n",
    "- Keep responses concise but informative (2-3 sentences)\n",
    "- Use technical terms but explain them when needed\n",
    "- Focus on practical applications\n",
    "- If unsure, acknowledge limitations\n",
    "\"\"\"\n",
    "\n",
    "# Create chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Test the template\n",
    "test_question = \"What is porosity?\"\n",
    "formatted_prompt = prompt_template.format_messages(question=test_question)\n",
    "print(\"Formatted prompt:\")\n",
    "for message in formatted_prompt:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating Modern Chains with LCEL\n",
    "\n",
    "Modern LangChain uses LCEL (LangChain Expression Language) for composing chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chain using LCEL\n",
    "simple_chain = prompt_template | chat_model | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "print(\"=== Testing Simple Chain ===\")\n",
    "response = simple_chain.invoke({\"question\": \"What is the difference between porosity and permeability?\"})\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.1 Low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "full_prompt = \"\"\n",
    "for msg in formatted_prompt:\n",
    "    if msg.type == \"system\":\n",
    "        full_prompt += f\"[SYSTEM]\\n{msg.content}\\n\"\n",
    "    elif msg.type == \"human\":\n",
    "        full_prompt += f\"[USER]\\n{msg.content}\\n\"\n",
    "\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Stream tokens as they are generated\n",
    "model.generate(**inputs, streamer=streamer, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple geoscience questions\n",
    "test_questions = [\n",
    "    \"What is seismic resolution?\",\n",
    "    \"How do P-waves differ from S-waves?\",\n",
    "    \"What factors affect hydrocarbon migration?\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Multiple Questions ===\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    response = simple_chain.invoke({\"question\": question})\n",
    "    print(f\"   Answer: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding Modern Conversational Memory\n",
    "\n",
    "Modern LangChain uses RunnableWithMessageHistory for conversation management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversational prompt template with history\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create the conversational chain\n",
    "conversational_chain = conversational_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# Store for conversation histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create conversational chain with memory\n",
    "conversational_with_memory = RunnableWithMessageHistory(\n",
    "    conversational_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"Conversational chain with memory created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversational memory\n",
    "print(\"=== Testing Conversational Memory ===\")\n",
    "\n",
    "session_config = {\"configurable\": {\"session_id\": \"test_session\"}}\n",
    "\n",
    "# First question\n",
    "response1 = conversational_with_memory.invoke(\n",
    "    {\"question\": \"What is seismic inversion?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q1: What is seismic inversion?\")\n",
    "print(f\"A1: {response1}\")\n",
    "print()\n",
    "\n",
    "# Follow-up question that refers to previous context\n",
    "response2 = conversational_with_memory.invoke(\n",
    "    {\"question\": \"What are the main types of this technique?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q2: What are the main types of this technique?\")\n",
    "print(f\"A2: {response2}\")\n",
    "print()\n",
    "\n",
    "# Another follow-up\n",
    "response3 = conversational_with_memory.invoke(\n",
    "    {\"question\": \"Which type is most commonly used in the industry?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q3: Which type is most commonly used in the industry?\")\n",
    "print(f\"A3: {response3}\")\n",
    "print()\n",
    "\n",
    "# Check memory content\n",
    "print(\"=== Current Memory ===\")\n",
    "history = get_session_history(\"test_session\")\n",
    "for message in history.messages:\n",
    "    print(f\"{message.type}: {message.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Modern Chat Agent Class\n",
    "\n",
    "| Model         | Size    | Specialization                  | Local Use | API Access | Cost (if applicable)      |\n",
    "|---------------|---------|---------------------------------|-----------|------------|--------------------------|\n",
    "| K2            | ~7B     | Geoscience instruction-tuned    | ✅ Yes    | ❌ No      | Free                     |\n",
    "| GeoGalactica  | 30B     | Geoscience-specific LLM         | ❌ No     | ❌ No      | N/A                      |\n",
    "| Galactica     | 120B    | General scientific knowledge    | ❌ No     | ❌ No      | N/A                      |\n",
    "| SciDFM        | ~5.6B   | Scientific reasoning (multi-domain) | ✅ Yes | ❌ No      | Free                     |\n",
    "| OceanGPT      | ~7B     | Ocean science tasks             | ✅ Yes    | ❌ No      | Free                     |\n",
    "| ClimateBERT   | ~6B     | Climate-related text classification | ✅ Yes | ❌ No      | Free                     |\n",
    "| GeoLM         | ~6B     | Geography-specific entity tasks | ✅ Yes    | ❌ No      | Free                     |\n",
    "| SpaBERT       | ~6B     | Spatial language understanding  | ✅ Yes    | ❌ No      | Free                     |\n",
    "| LLAMA-2 (e.g., 70B) | 70B+ | General-purpose LLMs         | ❌ No     | ✅ Yes      | Paid (Hugging Face Pro)  |\n",
    "\n",
    "Let's create a robust chat agent using modern LangChain patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import uuid\n",
    "\n",
    "class ModernGeoscienceChatAgent:\n",
    "    def __init__(self, chat_model):\n",
    "        self.chat_model = chat_model\n",
    "        self.store = {}\n",
    "        \n",
    "        # Enhanced system prompt\n",
    "        self.system_prompt = \"\"\"\n",
    "You are Dr. GeoBot, a friendly and knowledgeable geoscience expert specializing in:\n",
    "- Geophysics and seismic interpretation\n",
    "- Petroleum geology and reservoir engineering  \n",
    "- Well logging and formation evaluation\n",
    "- Hydrocarbon exploration and production\n",
    "- Geomechanics and drilling engineering\n",
    "\n",
    "Guidelines:\n",
    "- Provide accurate, helpful answers about geoscience topics\n",
    "- Use technical terms but explain them when needed\n",
    "- Be conversational and engaging\n",
    "- Keep responses focused and informative\n",
    "- If unsure, acknowledge limitations honestly\n",
    "- Reference previous conversation when relevant\n",
    "\"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # Create chain\n",
    "        self.chain = self.prompt | self.chat_model | StrOutputParser()\n",
    "        \n",
    "        # Create conversational chain with memory\n",
    "        self.conversational_chain = RunnableWithMessageHistory(\n",
    "            self.chain,\n",
    "            self.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "    \n",
    "    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "    \n",
    "    def chat(self, question: str, session_id: str = \"default\") -> str:\n",
    "        \"\"\"Process a question and return a response\"\"\"\n",
    "        try:\n",
    "            config = {\"configurable\": {\"session_id\": session_id}}\n",
    "            response = self.conversational_chain.invoke(\n",
    "                {\"question\": question},\n",
    "                config=config\n",
    "            )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def clear_memory(self, session_id: str = \"default\"):\n",
    "        \"\"\"Clear conversation history for a session\"\"\"\n",
    "        if session_id in self.store:\n",
    "            self.store[session_id].clear()\n",
    "    \n",
    "    def get_history(self, session_id: str = \"default\") -> list:\n",
    "        \"\"\"Get conversation history for a session\"\"\"\n",
    "        if session_id in self.store:\n",
    "            return self.store[session_id].messages\n",
    "        return []\n",
    "    \n",
    "    def create_new_session(self) -> str:\n",
    "        \"\"\"Create a new conversation session\"\"\"\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "# Create the modern chat agent\n",
    "chat_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "print(\"Modern GeoscienceChatAgent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the modern chat agent\n",
    "print(\"=== Testing Modern GeoscienceChatAgent ===\")\n",
    "\n",
    "# Test conversation\n",
    "questions = [\n",
    "    \"Hello! Can you explain what you specialize in?\",\n",
    "    \"What is the difference between conventional and unconventional reservoirs?\",\n",
    "    \"How do geophysicists use seismic data to find oil?\",\n",
    "    \"What role does well logging play in this process?\"\n",
    "]\n",
    "\n",
    "session_id = chat_agent.create_new_session()\n",
    "print(f\"Created session: {session_id[:8]}...\\n\")\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"{i}. Human: {question}\")\n",
    "    response = chat_agent.chat(question, session_id)\n",
    "    print(f\"   Dr. GeoBot: {response}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a Modern Gradio Interface\n",
    "\n",
    "Let's create an improved web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Create a new chat agent for the interface\n",
    "gradio_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "\n",
    "# Global session management\n",
    "current_session = gradio_agent.create_new_session()\n",
    "\n",
    "def respond(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Process user message and return bot response\n",
    "    \"\"\"\n",
    "    global current_session\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    # Get response from agent\n",
    "    bot_response = gradio_agent.chat(message, current_session)\n",
    "    \n",
    "    # Add to chat history\n",
    "    history.append((message, bot_response))\n",
    "    \n",
    "    return \"\", history\n",
    "\n",
    "def clear_conversation() -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Clear conversation history and start new session\n",
    "    \"\"\"\n",
    "    global current_session\n",
    "    gradio_agent.clear_memory(current_session)\n",
    "    current_session = gradio_agent.create_new_session()\n",
    "    return []\n",
    "\n",
    "def load_example(example: str) -> str:\n",
    "    \"\"\"\n",
    "    Load example question into the textbox\n",
    "    \"\"\"\n",
    "    return example\n",
    "\n",
    "# Create modern Gradio interface\n",
    "with gr.Blocks(\n",
    "    title=\"Dr. GeoBot - Advanced Geoscience Chat Assistant\",\n",
    "    theme=gr.themes.Soft()\n",
    ") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🌍 Dr. GeoBot - Your Advanced Geoscience Expert\n",
    "    \n",
    "    I'm an AI geoscience expert powered by modern LangChain. Ask me about:\n",
    "    \n",
    "    | **Geophysics** | **Petroleum Engineering** | **Well Logging** |\n",
    "    |---|---|---|\n",
    "    | Seismic interpretation | Reservoir characterization | Formation evaluation |\n",
    "    | Gravity & magnetics | Hydrocarbon systems | Petrophysics |\n",
    "    | Electromagnetics | Production optimization | Log analysis |\n",
    "    \n",
    "    💡 *I remember our conversation, so feel free to ask follow-up questions!*\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                value=[],\n",
    "                height=500,\n",
    "                show_label=False,\n",
    "                bubble_full_width=False\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Ask me about geoscience topics...\",\n",
    "                    show_label=False,\n",
    "                    scale=4,\n",
    "                    container=False\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send 📤\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"🗑️ Clear Chat\", variant=\"secondary\")\n",
    "                \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### 💡 Example Questions\")\n",
    "            \n",
    "            example_questions = [\n",
    "                \"What is seismic inversion?\",\n",
    "                \"Explain porosity vs permeability\",\n",
    "                \"How do P-waves and S-waves differ?\",\n",
    "                \"What is reservoir characterization?\",\n",
    "                \"How does well logging work?\",\n",
    "                \"What are the challenges in unconventional reservoirs?\"\n",
    "            ]\n",
    "            \n",
    "            for question in example_questions:\n",
    "                example_btn = gr.Button(\n",
    "                    question,\n",
    "                    variant=\"secondary\",\n",
    "                    size=\"sm\"\n",
    "                )\n",
    "                example_btn.click(\n",
    "                    load_example,\n",
    "                    inputs=[gr.State(question)],\n",
    "                    outputs=msg\n",
    "                )\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear_btn.click(clear_conversation, outputs=chatbot)\n",
    "\n",
    "# Launch the interface\n",
    "print(\"Launching modern Gradio interface...\")\n",
    "demo.launch(share=True, show_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise: Advanced Geoscience Conversations\n",
    "\n",
    "Now let's test our modern chat agent with complex geoscience scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh agent for exercises\n",
    "exercise_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "\n",
    "# Advanced conversation scenarios\n",
    "scenarios = {\n",
    "    \"Reservoir Engineering\": [\n",
    "        \"I'm working on a carbonate reservoir with high porosity but low permeability. What could be causing this?\",\n",
    "        \"What completion techniques would you recommend?\",\n",
    "        \"How would you evaluate the success of these techniques?\"\n",
    "    ],\n",
    "    \"Seismic Interpretation\": [\n",
    "        \"I'm seeing some unusual amplitude anomalies in my seismic data. What could these indicate?\",\n",
    "        \"How can I distinguish between hydrocarbon effects and lithology changes?\",\n",
    "        \"What additional data would help confirm my interpretation?\"\n",
    "    ],\n",
    "    \"Well Logging\": [\n",
    "        \"My resistivity logs show high values but my neutron-density logs suggest high porosity. How do I reconcile this?\",\n",
    "        \"What could cause this apparent contradiction?\",\n",
    "        \"Which additional logs would help clarify the situation?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== Advanced Exercise Scenarios ===\")\n",
    "print(\"Choose a scenario to explore:\")\n",
    "for i, scenario in enumerate(scenarios.keys(), 1):\n",
    "    print(f\"{i}. {scenario}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complete scenario conversation\n",
    "def run_scenario(scenario_name: str):\n",
    "    print(f\"\\n=== {scenario_name} Scenario ===\")\n",
    "    session_id = exercise_agent.create_new_session()\n",
    "    \n",
    "    questions = scenarios[scenario_name]\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n{i}. Expert: {question}\")\n",
    "        response = exercise_agent.chat(question, session_id)\n",
    "        print(f\"   Dr. GeoBot: {response}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    return session_id\n",
    "\n",
    "# Run the reservoir engineering scenario\n",
    "reservoir_session = run_scenario(\"Reservoir Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exercise function\n",
    "def interactive_exercise():\n",
    "    print(\"🌍 Welcome to Dr. GeoBot Advanced Exercise!\")\n",
    "    print(\"Available commands:\")\n",
    "    print(\"  - Type 'scenario <number>' to start a scenario (1-3)\")\n",
    "    print(\"  - Type 'new' to start fresh conversation\")\n",
    "    print(\"  - Type 'history' to see conversation history\")\n",
    "    print(\"  - Type 'quit' to exit\\n\")\n",
    "    \n",
    "    current_session = exercise_agent.create_new_session()\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Dr. GeoBot: Goodbye! Happy exploring! 🌍\")\n",
    "            break\n",
    "        elif user_input.lower() == 'new':\n",
    "            current_session = exercise_agent.create_new_session()\n",
    "            print(\"🔄 Started new conversation session\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'history':\n",
    "            history = exercise_agent.get_history(current_session)\n",
    "            print(f\"📚 Conversation history ({len(history)} messages):\")\n",
    "            for msg in history[-6:]:  # Show last 6 messages\n",
    "                print(f\"  {msg.type}: {msg.content[:60]}...\")\n",
    "            continue\n",
    "        elif user_input.lower().startswith('scenario'):\n",
    "            try:\n",
    "                scenario_num = int(user_input.split()[1])\n",
    "                scenario_names = list(scenarios.keys())\n",
    "                if 1 <= scenario_num <= len(scenario_names):\n",
    "                    current_session = run_scenario(scenario_names[scenario_num-1])\n",
    "                else:\n",
    "                    print(f\"Please choose scenario 1-{len(scenario_names)}\")\n",
    "            except (IndexError, ValueError):\n",
    "                print(\"Usage: scenario <number>\")\n",
    "            continue\n",
    "        \n",
    "        if user_input:\n",
    "            response = exercise_agent.chat(user_input, current_session)\n",
    "            print(f\"Dr. GeoBot: {response}\\n\")\n",
    "\n",
    "# Uncomment to start interactive exercise\n",
    "# interactive_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we built a modern conversational AI agent using current LangChain best practices:\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Modern LangChain Architecture**:\n",
    "   - ✅ LCEL (LangChain Expression Language) for chain composition\n",
    "   - ✅ ChatModels and structured message handling\n",
    "   - ✅ RunnableWithMessageHistory for conversation management\n",
    "   - ✅ Proper session management and memory handling\n",
    "\n",
    "2. **Advanced Features**:\n",
    "   - ✅ Multi-session conversation support\n",
    "   - ✅ Structured prompt templates with MessagesPlaceholder\n",
    "   - ✅ Modern error handling and response parsing\n",
    "   - ✅ Session-based memory management\n",
    "\n",
    "3. **Geoscience Applications**:\n",
    "   - ✅ Domain-specific expert persona (Dr. GeoBot)\n",
    "   - ✅ Technical geoscience conversation scenarios\n",
    "   - ✅ Context-aware follow-up questions\n",
    "   - ✅ Multi-topic expertise coverage\n",
    "\n",
    "4. **Modern UI/UX**:\n",
    "   - ✅ Enhanced Gradio interface with themes\n",
    "   - ✅ Example question buttons for easy interaction\n",
    "   - ✅ Improved conversation display and management\n",
    "   - ✅ Session management and conversation clearing\n",
    "\n",
    "### Key Improvements Over Legacy LangChain:\n",
    "\n",
    "| **Legacy** | **Modern** |\n",
    "|---|---|\n",
    "| `LLMChain` | LCEL (`|` operator) |\n",
    "| `ConversationBufferMemory` | `RunnableWithMessageHistory` |\n",
    "| Manual prompt formatting | `ChatPromptTemplate` |\n",
    "| Basic error handling | Structured exception management |\n",
    "| Single conversation | Multi-session support |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Module 1.3**: Add RAG (Retrieval Augmented Generation) for factual accuracy\n",
    "- **Module 1.4**: Integrate external tools and function calling\n",
    "- **Session 2**: Fine-tune models on geoscience datasets\n",
    "- **Session 3**: Build specialized applications (seismic analysis, log interpretation)\n",
    "\n",
    "### Exercise Extensions:\n",
    "\n",
    "1. **Customize the Expert**: Modify the system prompt to create specialists (seismic interpreter, reservoir engineer, etc.)\n",
    "2. **Add Validation**: Implement response quality checking and topic relevance\n",
    "3. **Export Conversations**: Add functionality to save/load conversation sessions\n",
    "4. **Multi-Agent Setup**: Create multiple specialized agents for different domains\n",
    "5. **Integration**: Connect with geoscience APIs or databases for real-time data\n",
    "\n",
    "This modern implementation provides a solid foundation for building production-ready geoscience chat applications! 🌍"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
