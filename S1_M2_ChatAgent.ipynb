{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S1_M2_ChatAgent.ipynb)\n",
    "\n",
    "***\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Follow along by running each cell in order\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Make sure to run the environment setup cells first\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Wait for each installation to complete before proceeding\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if installations take a while - this is normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/SPE_GeoHackathon_2025/refs/heads/dev/spe_utils.txt -O spe_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=5 -i spe_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup [If running outside Colab]\n",
    "# !pip install langchain langchain-huggingface transformers torch gradio\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# # Retrieving the token is required to get access to HF hub\n",
    "# from google.colab import userdata\n",
    "# hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spe_utils.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spe_utils.data import (\n",
    "    GEOSCIENCE_TERMS,\n",
    "    GEOSCIENCE_QA_PAIRS,\n",
    "    CONVERSATION_STARTERS,\n",
    "    GEOPHYSICS_EXPERT_PROMPTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 01 // Module 02: First Chat Agent with LangChain\n",
    "\n",
    "In this module, we'll build our first conversational AI agent using LangChain. We'll create a geoscience-focused chatbot that can answer questions about geology, geophysics, and petroleum engineering concepts.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand LangChain fundamentals (LLM wrappers, prompt templates)\n",
    "- Build a simple Q&A chat agent with Hugging Face models\n",
    "- Add conversational memory to maintain context\n",
    "- Create an interactive Gradio interface\n",
    "- Apply the agent to geoscience conversations\n",
    "\n",
    "## 1. LangChain Basics\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides:\n",
    "- **LLM Wrappers**: Standardized interfaces for different models\n",
    "- **Prompt Templates**: Reusable, parameterized prompts\n",
    "- **Chains**: Sequences of operations with LLMs\n",
    "- **Memory**: Persistent conversation context\n",
    "- **Agents**: LLMs that can use tools and make decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting up the Language Model\n",
    "\n",
    "First, let's load a Hugging Face model and wrap it with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a slightly larger model for better conversational abilities\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "# Create HuggingFace pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Wrap with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating Prompt Templates\n",
    "\n",
    "Prompt templates allow us to create reusable, parameterized prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic prompt template for geoscience Q&A\n",
    "basic_template = \"\"\"\n",
    "You are a helpful geoscience expert assistant. Answer the following question clearly and concisely.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "basic_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=basic_template\n",
    ")\n",
    "\n",
    "# Test the template\n",
    "test_question = \"What is porosity?\"\n",
    "formatted_prompt = basic_prompt.format(question=test_question)\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sophisticated template with context\n",
    "geoscience_template = \"\"\"\n",
    "You are Dr. GeoBot, an expert geophysicist and petroleum engineer with 20 years of experience. \n",
    "You specialize in seismic interpretation, reservoir characterization, and hydrocarbon exploration.\n",
    "\n",
    "Context: You are helping students and professionals understand geoscience concepts.\n",
    "Keep your answers:\n",
    "- Technically accurate but accessible\n",
    "- Focused on practical applications\n",
    "- Around 2-3 sentences when possible\n",
    "\n",
    "Human: {question}\n",
    "Dr. GeoBot:\"\"\"\n",
    "\n",
    "geoscience_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=geoscience_template\n",
    ")\n",
    "\n",
    "# Test with a geoscience question\n",
    "geo_question = \"How does seismic inversion help in reservoir characterization?\"\n",
    "formatted_geo_prompt = geoscience_prompt.format(question=geo_question)\n",
    "print(\"Geoscience prompt:\")\n",
    "print(formatted_geo_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating LLM Chains\n",
    "\n",
    "Chains combine prompts and models for streamlined execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple LLM chain\n",
    "simple_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=geoscience_prompt,\n",
    "    verbose=True  # Shows the prompt being sent to the model\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "print(\"=== Testing Simple Chain ===\")\n",
    "response = simple_chain.invoke({\"question\": \"What is the difference between porosity and permeability?\"})\n",
    "print(f\"Response: {response['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple geoscience questions\n",
    "test_questions = [\n",
    "    \"What is seismic resolution?\",\n",
    "    \"How do P-waves differ from S-waves?\",\n",
    "    \"What factors affect hydrocarbon migration?\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Multiple Questions ===\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    response = simple_chain.invoke({\"question\": question})\n",
    "    print(f\"   Answer: {response['text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding Conversational Memory\n",
    "\n",
    "Memory allows our chatbot to remember previous conversations and maintain context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversational template with memory\n",
    "conversation_template = \"\"\"\n",
    "You are Dr. GeoBot, an expert geophysicist and petroleum engineer. You are having a conversation with a student or professional about geoscience topics.\n",
    "\n",
    "Previous conversation:\n",
    "{history}\n",
    "\n",
    "Current question: {question}\n",
    "Dr. GeoBot:\"\"\"\n",
    "\n",
    "conversation_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=conversation_template\n",
    ")\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "# Create conversational chain\n",
    "conversation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=conversation_prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Conversational chain created with memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversational memory\n",
    "print(\"=== Testing Conversational Memory ===\")\n",
    "\n",
    "# First question\n",
    "response1 = conversation_chain.invoke({\"question\": \"What is seismic inversion?\"})\n",
    "print(f\"Q1: What is seismic inversion?\")\n",
    "print(f\"A1: {response1['text']}\")\n",
    "print()\n",
    "\n",
    "# Follow-up question that refers to previous context\n",
    "response2 = conversation_chain.invoke({\"question\": \"What are the main types of this technique?\"})\n",
    "print(f\"Q2: What are the main types of this technique?\")\n",
    "print(f\"A2: {response2['text']}\")\n",
    "print()\n",
    "\n",
    "# Another follow-up\n",
    "response3 = conversation_chain.invoke({\"question\": \"Which type is most commonly used in the industry?\"})\n",
    "print(f\"Q3: Which type is most commonly used in the industry?\")\n",
    "print(f\"A3: {response3['text']}\")\n",
    "print()\n",
    "\n",
    "# Check memory content\n",
    "print(\"=== Current Memory ===\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Q&A Chat Agent\n",
    "\n",
    "Let's create a more robust chat agent with better response handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoscienceChatAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"history\",\n",
    "            input_key=\"question\"\n",
    "        )\n",
    "        \n",
    "        self.template = \"\"\"\n",
    "You are Dr. GeoBot, a friendly and knowledgeable geoscience expert specializing in:\n",
    "- Geophysics and seismic interpretation\n",
    "- Petroleum geology and reservoir engineering  \n",
    "- Well logging and formation evaluation\n",
    "- Hydrocarbon exploration and production\n",
    "\n",
    "Guidelines:\n",
    "- Provide accurate, helpful answers about geoscience topics\n",
    "- Use technical terms but explain them when needed\n",
    "- Be conversational and engaging\n",
    "- If unsure, acknowledge limitations\n",
    "\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "Human: {question}\n",
    "Dr. GeoBot:\"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"question\"],\n",
    "            template=self.template\n",
    "        )\n",
    "        \n",
    "        self.chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt,\n",
    "            memory=self.memory\n",
    "        )\n",
    "    \n",
    "    def chat(self, question):\n",
    "        \"\"\"Process a question and return a response\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke({\"question\": question})\n",
    "            return response['text'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def get_history(self):\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.memory.buffer\n",
    "\n",
    "# Create the chat agent\n",
    "chat_agent = GeoscienceChatAgent(llm)\n",
    "print(\"GeoscienceChatAgent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chat agent\n",
    "print(\"=== Testing GeoscienceChatAgent ===\")\n",
    "\n",
    "# Test conversation\n",
    "questions = [\n",
    "    \"Hello! Can you explain what you specialize in?\",\n",
    "    \"What is the difference between conventional and unconventional reservoirs?\",\n",
    "    \"How do geophysicists use seismic data to find oil?\",\n",
    "    \"What role does well logging play in this process?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{i}. Human: {question}\")\n",
    "    response = chat_agent.chat(question)\n",
    "    print(f\"   Dr. GeoBot: {response}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating an Interactive Gradio Interface\n",
    "\n",
    "Let's create a web-based chat interface using Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "# Create a new chat agent for the interface\n",
    "gradio_agent = GeoscienceChatAgent(llm)\n",
    "\n",
    "def respond(message, history):\n",
    "    \"\"\"\n",
    "    Process user message and return bot response\n",
    "    \"\"\"\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    # Get response from agent\n",
    "    bot_response = gradio_agent.chat(message)\n",
    "    \n",
    "    # Add to chat history\n",
    "    history.append((message, bot_response))\n",
    "    \n",
    "    return \"\", history\n",
    "\n",
    "def clear_conversation():\n",
    "    \"\"\"\n",
    "    Clear conversation history\n",
    "    \"\"\"\n",
    "    gradio_agent.clear_memory()\n",
    "    return []\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Dr. GeoBot - Geoscience Chat Assistant\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üåç Dr. GeoBot - Your Geoscience Expert\n",
    "    \n",
    "    Ask me anything about:\n",
    "    - **Geophysics** (seismic interpretation, gravity, magnetics)\n",
    "    - **Petroleum Geology** (reservoir characterization, hydrocarbon systems)\n",
    "    - **Well Logging** (formation evaluation, petrophysics)\n",
    "    - **Exploration** (prospect evaluation, risk assessment)\n",
    "    \n",
    "    *Try starting with: \"What is seismic inversion?\" or \"Explain porosity vs permeability\"*\n",
    "    \"\"\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        value=[],\n",
    "        height=400,\n",
    "        show_label=False\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            placeholder=\"Ask me about geoscience topics...\",\n",
    "            show_label=False,\n",
    "            scale=4\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send\", scale=1)\n",
    "    \n",
    "    with gr.Row():\n",
    "        clear_btn = gr.Button(\"Clear Conversation\")\n",
    "        \n",
    "    # Example questions\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"What is seismic inversion?\",\n",
    "            \"Explain the difference between porosity and permeability\",\n",
    "            \"How do P-waves and S-waves differ?\",\n",
    "            \"What is reservoir characterization?\",\n",
    "            \"How does well logging help in formation evaluation?\"\n",
    "        ],\n",
    "        inputs=msg\n",
    "    )\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear_btn.click(clear_conversation, outputs=chatbot)\n",
    "\n",
    "# Launch the interface\n",
    "print(\"Launching Gradio interface...\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise: Chat with the Agent about Geoscience Concepts\n",
    "\n",
    "Now it's time to interact with your chat agent! Try the following conversation scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh agent for the exercise\n",
    "exercise_agent = GeoscienceChatAgent(llm)\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat function for the exercise\n",
    "    \"\"\"\n",
    "    print(\"üåç Welcome to Dr. GeoBot! Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Dr. GeoBot: Goodbye! Happy exploring! üåç\")\n",
    "            break\n",
    "            \n",
    "        if user_input.strip():\n",
    "            response = exercise_agent.chat(user_input)\n",
    "            print(f\"Dr. GeoBot: {response}\\n\")\n",
    "\n",
    "# Suggested conversation starters\n",
    "print(\"=== Exercise: Chat with Dr. GeoBot ===\")\n",
    "print(\"\\nSuggested conversation topics:\")\n",
    "print(\"1. Start with: 'What is your expertise in geophysics?'\")\n",
    "print(\"2. Ask about: 'How does seismic data help find oil and gas?'\")\n",
    "print(\"3. Follow up: 'What are the main challenges in seismic interpretation?'\")\n",
    "print(\"4. Explore: 'How do reservoir properties affect production?'\")\n",
    "print(\"5. Discuss: 'What is the role of machine learning in geoscience?'\")\n",
    "\n",
    "# Uncomment the next line to start interactive chat\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Pre-scripted conversation for demonstration\n",
    "demonstration_questions = [\n",
    "    \"Hello Dr. GeoBot! What can you help me with?\",\n",
    "    \"I'm studying reservoir engineering. Can you explain what affects hydrocarbon recovery?\",\n",
    "    \"How do porosity and permeability work together?\",\n",
    "    \"What techniques can we use to measure these properties?\",\n",
    "    \"How reliable are these measurements?\",\n",
    "    \"Thank you for the explanation!\"\n",
    "]\n",
    "\n",
    "print(\"=== Demonstration Conversation ===\")\n",
    "for i, question in enumerate(demonstration_questions, 1):\n",
    "    print(f\"\\n{i}. You: {question}\")\n",
    "    response = exercise_agent.chat(question)\n",
    "    print(f\"   Dr. GeoBot: {response}\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Features and Improvements\n",
    "\n",
    "Let's explore some advanced features to make our chat agent even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced chat agent with better response formatting\n",
    "class AdvancedGeoscienceChatAgent(GeoscienceChatAgent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm)\n",
    "        \n",
    "        # Enhanced template with better structure\n",
    "        self.template = \"\"\"\n",
    "You are Dr. GeoBot, an expert geoscientist with extensive knowledge in:\n",
    "‚Ä¢ Geophysics (seismic, gravity, magnetics, electromagnetics)\n",
    "‚Ä¢ Petroleum geology and reservoir engineering\n",
    "‚Ä¢ Well logging and formation evaluation\n",
    "‚Ä¢ Hydrocarbon exploration and production\n",
    "‚Ä¢ Geomechanics and drilling engineering\n",
    "\n",
    "Instructions:\n",
    "- Provide accurate, well-structured answers\n",
    "- Use bullet points or numbering when listing items\n",
    "- Include practical examples when relevant\n",
    "- Admit when you're uncertain\n",
    "- Keep responses focused and informative\n",
    "\n",
    "Previous conversation:\n",
    "{history}\n",
    "\n",
    "Human: {question}\n",
    "Dr. GeoBot:\"\"\"\n",
    "        \n",
    "        # Update the prompt\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"question\"],\n",
    "            template=self.template\n",
    "        )\n",
    "        \n",
    "        # Recreate the chain\n",
    "        self.chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt,\n",
    "            memory=self.memory\n",
    "        )\n",
    "    \n",
    "    def chat_with_context(self, question, context=None):\n",
    "        \"\"\"\n",
    "        Chat with additional context\n",
    "        \"\"\"\n",
    "        if context:\n",
    "            enhanced_question = f\"Context: {context}\\n\\nQuestion: {question}\"\n",
    "        else:\n",
    "            enhanced_question = question\n",
    "            \n",
    "        return self.chat(enhanced_question)\n",
    "    \n",
    "    def get_conversation_summary(self):\n",
    "        \"\"\"\n",
    "        Get a summary of the current conversation\n",
    "        \"\"\"\n",
    "        history = self.get_history()\n",
    "        if not history:\n",
    "            return \"No conversation history yet.\"\n",
    "        \n",
    "        # Simple summary based on history length\n",
    "        lines = history.split('\\n')\n",
    "        human_questions = [line for line in lines if line.startswith('Human:')]\n",
    "        \n",
    "        return f\"Conversation includes {len(human_questions)} questions covering various geoscience topics.\"\n",
    "\n",
    "# Create advanced agent\n",
    "advanced_agent = AdvancedGeoscienceChatAgent(llm)\n",
    "print(\"Advanced GeoscienceChatAgent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced features\n",
    "print(\"=== Testing Advanced Features ===\")\n",
    "\n",
    "# Test with context\n",
    "context = \"I'm working on a carbonate reservoir in the Middle East with high porosity but low permeability.\"\n",
    "question = \"What completion techniques should I consider?\"\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Question: {question}\")\n",
    "response = advanced_agent.chat_with_context(question, context)\n",
    "print(f\"Dr. GeoBot: {response}\\n\")\n",
    "\n",
    "# Follow-up question\n",
    "followup = \"What are the risks associated with these techniques?\"\n",
    "print(f\"Follow-up: {followup}\")\n",
    "response2 = advanced_agent.chat(followup)\n",
    "print(f\"Dr. GeoBot: {response2}\\n\")\n",
    "\n",
    "# Check conversation summary\n",
    "summary = advanced_agent.get_conversation_summary()\n",
    "print(f\"Conversation summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we successfully built a conversational AI agent for geoscience applications:\n",
    "\n",
    "### What We Learned:\n",
    "1. **LangChain Fundamentals**:\n",
    "   - LLM wrappers for standardized model interfaces\n",
    "   - Prompt templates for reusable, parameterized prompts\n",
    "   - Chains for combining prompts and models\n",
    "\n",
    "2. **Conversational Memory**:\n",
    "   - ConversationBufferMemory for maintaining context\n",
    "   - How memory enables follow-up questions\n",
    "   - Managing conversation history\n",
    "\n",
    "3. **Chat Agent Development**:\n",
    "   - Building a specialized geoscience chatbot\n",
    "   - Error handling and response formatting\n",
    "   - Creating interactive interfaces with Gradio\n",
    "\n",
    "4. **Geoscience Applications**:\n",
    "   - Domain-specific prompting strategies\n",
    "   - Technical terminology and explanations\n",
    "   - Contextual conversations about complex topics\n",
    "\n",
    "### Key Features Implemented:\n",
    "- ‚úÖ Conversational memory for context retention\n",
    "- ‚úÖ Specialized geoscience knowledge prompting\n",
    "- ‚úÖ Interactive web interface with Gradio\n",
    "- ‚úÖ Error handling and response formatting\n",
    "- ‚úÖ Multiple conversation scenarios\n",
    "\n",
    "### Next Steps:\n",
    "- **Module 1.3**: Add retrieval capabilities (RAG) for factual accuracy\n",
    "- **Module 1.4**: Integrate external tools and APIs\n",
    "- **Session 2**: Fine-tune models on domain-specific data\n",
    "- **Session 3**: Build specialized applications (log analysis, seismic interpretation)\n",
    "\n",
    "### Exercise Suggestions:\n",
    "1. Modify the prompts to focus on your specific area of expertise\n",
    "2. Add personality traits to make the bot more engaging\n",
    "3. Implement conversation saving/loading functionality\n",
    "4. Create specialized templates for different types of questions\n",
    "5. Add validation to ensure responses stay on topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}