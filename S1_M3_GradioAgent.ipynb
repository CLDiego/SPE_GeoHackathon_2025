{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c377e703",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S1_M2_ChatAgent.ipynb)\n",
    "\n",
    "***\n",
    "# Session 01 // Module 03: Agent Interfaces\n",
    "\n",
    "### What is an agent?\n",
    "\n",
    "An agent is an LLM-driven decision-maker that can plan steps and choose tools to reach a goal. Instead of a fixed, linear chain, an agent decides what to do next at runtime.\n",
    "\n",
    "- Core pieces:\n",
    "  - LLM: the “brain” that reasons about the task.\n",
    "  - Prompt/policy: instructions + scratchpad that guide the LLM’s decisions.\n",
    "  - Tools: callable functions/APIs (e.g., web search, calculators, vector stores).\n",
    "  - Memory (optional): prior turns or facts that persist across steps.\n",
    "  - Executor/loop: runs the think → act (tool) → observe cycle until the agent decides it’s done.\n",
    "\n",
    "- When to use:\n",
    "  - Multi-step tasks with uncertainty or branching.\n",
    "  - When external tools/data are needed (RAG, code, search, math).\n",
    "  - When you want the model to decide the next best action dynamically.\n",
    "\n",
    "- Chain vs Agent:\n",
    "  - Chain: predefined flow (prompt → model → parser).\n",
    "  - Agent: dynamic flow chosen by the model at runtime.\n",
    "\n",
    "In this notebook, we build a memory-enabled chat workflow (a building block for agents). You can later add tools and wrap with an Agent executor to turn it into a full tool-using agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment setup\n",
    "!pip -q install langchain langchain-core langchain-community langchain-huggingface torch gradio\n",
    "!pip -q install bitsandbytes==0.46.0 transformers==4.48.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# Retrieving the token is required to get access to HF hub\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc994d9e",
   "metadata": {},
   "source": [
    "## 7. Designing a simple agent class\n",
    "\n",
    "We wrap the chain and memory plumbing into a class to:\n",
    "- Encapsulate system prompts and chains.\n",
    "- Offer simple methods (chat, clear_memory, get_history, create_new_session).\n",
    "- Swap models without changing the interface.\n",
    "\n",
    "Class design tips:\n",
    "- Keep system prompts centralized and editable.\n",
    "- Return clean strings to the UI layer.\n",
    "- Add basic try/except for robust demos.\n",
    "\n",
    "```python\n",
    "response = agent.chat(\"What role does well logging play?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import uuid\n",
    "\n",
    "class ModernGeoscienceChatAgent:\n",
    "    def __init__(self, chat_model):\n",
    "        self.chat_model = chat_model\n",
    "        self.store = {}\n",
    "        \n",
    "        # Enhanced system prompt\n",
    "        self.system_prompt = \"\"\"\n",
    "You are Dr. GeoBot, a friendly and knowledgeable geoscience expert specializing in:\n",
    "- Geophysics and seismic interpretation\n",
    "- Petroleum geology and reservoir engineering  \n",
    "- Well logging and formation evaluation\n",
    "- Hydrocarbon exploration and production\n",
    "- Geomechanics and drilling engineering\n",
    "\n",
    "Guidelines:\n",
    "- Provide accurate, helpful answers about geoscience topics\n",
    "- Use technical terms but explain them when needed\n",
    "- Be conversational and engaging\n",
    "- Keep responses focused and informative\n",
    "- If unsure, acknowledge limitations honestly\n",
    "- Reference previous conversation when relevant\n",
    "\"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # Create chain\n",
    "        self.chain = self.prompt | self.chat_model | StrOutputParser()\n",
    "        \n",
    "        # Create conversational chain with memory\n",
    "        self.conversational_chain = RunnableWithMessageHistory(\n",
    "            self.chain,\n",
    "            self.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "    \n",
    "    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "    \n",
    "    def chat(self, question: str, session_id: str = \"default\") -> str:\n",
    "        \"\"\"Process a question and return a response\"\"\"\n",
    "        try:\n",
    "            config = {\"configurable\": {\"session_id\": session_id}}\n",
    "            response = self.conversational_chain.invoke(\n",
    "                {\"question\": question},\n",
    "                config=config\n",
    "            )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def clear_memory(self, session_id: str = \"default\"):\n",
    "        \"\"\"Clear conversation history for a session\"\"\"\n",
    "        if session_id in self.store:\n",
    "            self.store[session_id].clear()\n",
    "    \n",
    "    def get_history(self, session_id: str = \"default\") -> list:\n",
    "        \"\"\"Get conversation history for a session\"\"\"\n",
    "        if session_id in self.store:\n",
    "            return self.store[session_id].messages\n",
    "        return []\n",
    "    \n",
    "    def create_new_session(self) -> str:\n",
    "        \"\"\"Create a new conversation session\"\"\"\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "# Create the modern chat agent\n",
    "chat_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "print(\"Modern GeoscienceChatAgent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the modern chat agent\n",
    "print(\"=== Testing Modern GeoscienceChatAgent ===\")\n",
    "\n",
    "# Test conversation\n",
    "questions = [\n",
    "    \"Hello! Can you explain what you specialize in?\",\n",
    "    \"What is the difference between conventional and unconventional reservoirs?\",\n",
    "    \"How do geophysicists use seismic data to find oil?\",\n",
    "    \"What role does well logging play in this process?\"\n",
    "]\n",
    "\n",
    "session_id = chat_agent.create_new_session()\n",
    "print(f\"Created session: {session_id[:8]}...\\n\")\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"{i}. Human: {question}\")\n",
    "    response = chat_agent.chat(question, session_id)\n",
    "    print(f\"   Dr. GeoBot: {response}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d8081",
   "metadata": {},
   "source": [
    "## 8. Building a Gradio UI\n",
    "\n",
    "We expose the agent through a browser-based chat.\n",
    "\n",
    "Core components:\n",
    "- gr.Blocks: Page/layout container.\n",
    "- gr.Chatbot: Conversation pane (list of (user, bot) tuples).\n",
    "- gr.Textbox: Input field for user messages.\n",
    "- gr.Button: Send, Clear, Load examples.\n",
    "- Events: .submit and .click wire UI to Python callbacks.\n",
    "\n",
    "UX tips:\n",
    "- Keep answers concise; use memory to reduce repetition.\n",
    "- Provide example prompts to guide first-time users.\n",
    "- Add a “Clear chat” to reset session/memory.\n",
    "\n",
    "```python\n",
    "def respond(message, history):\n",
    "    reply = agent.chat(message, session_id)\n",
    "    history.append((message, reply))\n",
    "    return \"\", history\n",
    "\n",
    "msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "clear_btn.click(lambda: agent.clear_memory(session_id) or [], outputs=chatbot)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Create a new chat agent for the interface\n",
    "gradio_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "\n",
    "# Global session management\n",
    "current_session = gradio_agent.create_new_session()\n",
    "\n",
    "def respond(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Process user message and return bot response\n",
    "    \"\"\"\n",
    "    global current_session\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    # Get response from agent\n",
    "    bot_response = gradio_agent.chat(message, current_session)\n",
    "    \n",
    "    # Add to chat history\n",
    "    history.append((message, bot_response))\n",
    "    \n",
    "    return \"\", history\n",
    "\n",
    "def clear_conversation() -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Clear conversation history and start new session\n",
    "    \"\"\"\n",
    "    global current_session\n",
    "    gradio_agent.clear_memory(current_session)\n",
    "    current_session = gradio_agent.create_new_session()\n",
    "    return []\n",
    "\n",
    "def load_example(example: str) -> str:\n",
    "    \"\"\"\n",
    "    Load example question into the textbox\n",
    "    \"\"\"\n",
    "    return example\n",
    "\n",
    "# Create modern Gradio interface\n",
    "with gr.Blocks(\n",
    "    title=\"Dr. GeoBot - Advanced Geoscience Chat Assistant\",\n",
    "    theme=gr.themes.Soft()\n",
    ") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🌍 Dr. GeoBot - Your Advanced Geoscience Expert\n",
    "    \n",
    "    I'm an AI geoscience expert powered by modern LangChain. Ask me about:\n",
    "    \n",
    "    | **Geophysics** | **Petroleum Engineering** | **Well Logging** |\n",
    "    |---|---|---|\n",
    "    | Seismic interpretation | Reservoir characterization | Formation evaluation |\n",
    "    | Gravity & magnetics | Hydrocarbon systems | Petrophysics |\n",
    "    | Electromagnetics | Production optimization | Log analysis |\n",
    "    \n",
    "    💡 *I remember our conversation, so feel free to ask follow-up questions!*\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                value=[],\n",
    "                height=500,\n",
    "                show_label=False,\n",
    "                bubble_full_width=False\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Ask me about geoscience topics...\",\n",
    "                    show_label=False,\n",
    "                    scale=4,\n",
    "                    container=False\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send 📤\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"🗑️ Clear Chat\", variant=\"secondary\")\n",
    "                \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### 💡 Example Questions\")\n",
    "            \n",
    "            example_questions = [\n",
    "                \"What is seismic inversion?\",\n",
    "                \"Explain porosity vs permeability\",\n",
    "                \"How do P-waves and S-waves differ?\",\n",
    "                \"What is reservoir characterization?\",\n",
    "                \"How does well logging work?\",\n",
    "                \"What are the challenges in unconventional reservoirs?\"\n",
    "            ]\n",
    "            \n",
    "            for question in example_questions:\n",
    "                example_btn = gr.Button(\n",
    "                    question,\n",
    "                    variant=\"secondary\",\n",
    "                    size=\"sm\"\n",
    "                )\n",
    "                example_btn.click(\n",
    "                    load_example,\n",
    "                    inputs=[gr.State(question)],\n",
    "                    outputs=msg\n",
    "                )\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear_btn.click(clear_conversation, outputs=chatbot)\n",
    "\n",
    "# Launch the interface\n",
    "print(\"Launching modern Gradio interface...\")\n",
    "demo.launch(share=True, show_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d093b",
   "metadata": {},
   "source": [
    "## 11. Quick reference\n",
    "\n",
    "- Keep prompts short and specific; push style guidance to the system message.\n",
    "- temperature: 0.2–0.7 for helpful chat; higher for creativity.\n",
    "- max_new_tokens: 100–200 for chat; lower = faster.\n",
    "- Use LCEL to compose; add memory with RunnableWithMessageHistory.\n",
    "- Test locally with tiny models; switch to Colab GPU for 7–8B.\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
