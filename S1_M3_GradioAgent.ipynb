{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c377e703",
   "metadata": {},
   "source": [
    "![Workshop Banner](assets/S1_M3.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/main/S1_M3_GradioAgent.ipynb)\n",
    "\n",
    "***\n",
    "# Session 01 // Module 03: Agent Interfaces\n",
    "\n",
    "This module takes the chat pipeline built in Module 02 and ships it as a usable browser app with Gradio. You’ll learn how to wrap your LangChain chat workflow into a small agent-like class, attach lightweight memory, and create a polished UI with events and session management.\n",
    "\n",
    "***\n",
    "\n",
    "# 1. What is an agent?\n",
    "\n",
    "An agent is an LLM-driven decision-maker that plans steps and chooses tools to reach a goal. Instead of a fixed, linear chain, an agent decides what to do next at runtime (think → act → observe loop).\n",
    "\n",
    "- Core pieces:\n",
    "  - LLM (the “brain”)\n",
    "  - Prompt/policy (instructions + scratchpad)\n",
    "  - Tools (functions/APIs the model can call)\n",
    "  - Memory (optional; keeps context)\n",
    "  - Executor/loop (runs until done)\n",
    "\n",
    "- When to use:\n",
    "  - Multi-step tasks with branching/uncertainty\n",
    "  - When you need external tools/data (RAG, web, code, math)\n",
    "  - When the model should decide the next action\n",
    "\n",
    "- Chain vs Agent:\n",
    "  - Chain: predefined flow (prompt → model → parser)\n",
    "  - Agent: dynamic, tool-using loop decided at runtime\n",
    "\n",
    "In this notebook, we implement a memory-enabled chat workflow and expose it in a UI. This is the foundation you’ll later wrap with an Agent executor and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Environment setup\n",
    "!pip -q install langchain langchain-core langchain-community langchain-huggingface torch gradio\n",
    "!pip -q install requests bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# Retrieving the token is required to get access to HF hub\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc994d9e",
   "metadata": {},
   "source": [
    "# 2. Designing a simple agent class\n",
    "\n",
    "To simplify usage and integration, we design a simple `ChatAgent` class that wraps the LangChain components explored earlier.\n",
    "\n",
    "Key features:\n",
    "- Initialize with model, prompt, and memory.\n",
    "- Encapsulate system prompts and chains.\n",
    "- Offer simple methods (chat, clear_memory, get_history, create_new_session).\n",
    "- Swap models without changing the interface.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Class design tips:\n",
    "> - Keep system prompts centralized and editable.\n",
    "> - Return clean strings to the UI layer.\n",
    "> - Add basic try/except for robust demos.\n",
    "\n",
    "By the end we will have a reusable `ChatAgent` class that can be easily extended with tools and wrapped in an Agent executor later. To use it, simply create an instance and call the `chat` method with user input:\n",
    "\n",
    "```python\n",
    "response = agent.chat(\"What role does well logging play?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "import uuid\n",
    "\n",
    "class ModernGeoscienceChatAgent:\n",
    "    def __init__(self, chat_model: ChatHuggingFace):\n",
    "        self.chat_model = chat_model\n",
    "        self.store = {}\n",
    "        \n",
    "        # Enhanced system prompt\n",
    "        self.system_prompt = \"\"\"\n",
    "You are Dr. GeoBot, a friendly and knowledgeable geoscience expert specializing in:\n",
    "- Geophysics and seismic interpretation\n",
    "- Petroleum geology and reservoir engineering  \n",
    "- Well logging and formation evaluation\n",
    "- Hydrocarbon exploration and production\n",
    "- Geomechanics and drilling engineering\n",
    "\n",
    "Guidelines:\n",
    "- Provide accurate, helpful answers about geoscience topics\n",
    "- Use technical terms but explain them when needed\n",
    "- Be conversational and engaging\n",
    "- Keep responses focused and informative\n",
    "- If unsure, acknowledge limitations honestly\n",
    "- Reference previous conversation when relevant\n",
    "\"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        # Create chain\n",
    "        self.chain = self.prompt | self.chat_model | StrOutputParser()\n",
    "        \n",
    "        # Create conversational chain with memory\n",
    "        self.conversational_chain = RunnableWithMessageHistory(\n",
    "            self.chain,\n",
    "            self.get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "    \n",
    "    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "    \n",
    "    def chat(self, question: str, session_id: str = \"default\") -> str:\n",
    "        \"\"\"Process a question and return a response\"\"\"\n",
    "        try:\n",
    "            config = {\"configurable\": {\"session_id\": session_id}}\n",
    "            response = self.conversational_chain.invoke(\n",
    "                {\"question\": question},\n",
    "                config=config\n",
    "            )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def clear_memory(self, session_id: str = \"default\"):\n",
    "        \"\"\"Clear conversation history for a session\"\"\"\n",
    "        if session_id in self.store:\n",
    "            self.store[session_id].clear()\n",
    "    \n",
    "    def get_history(self, session_id: str = \"default\") -> list:\n",
    "        \"\"\"Get conversation history for a session\"\"\"\n",
    "        if session_id in self.store:\n",
    "            return self.store[session_id].messages\n",
    "        return []\n",
    "    \n",
    "    def create_new_session(self) -> str:\n",
    "        \"\"\"Create a new conversation session\"\"\"\n",
    "        return str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726933c",
   "metadata": {},
   "source": [
    "## 2.1 Model loading and decoding parameters\n",
    "\n",
    "We use a Transformers text-generation pipeline and pass it through LangChain.\n",
    "\n",
    "Common knobs:\n",
    "- `max_new_tokens`: cap generated tokens (100–200 for chat)\n",
    "- `temperature`: randomness (0.2–0.7 helpful; >0.9 creative)\n",
    "- `top_p` or `top_k`: sampling filters; use one for easier tuning\n",
    "- `repetition_penalty` or `no_repeat_ngram_size`: reduce loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a570419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.2,\n",
    "    do_sample=True, # Sampling enables more diverse outputs\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    return_full_text=False # The generated text will not include the prompt\n",
    ")\n",
    "\n",
    "# Create LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Create the modern chat agent\n",
    "chat_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "print(\"Modern GeoscienceChatAgent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the modern chat agent\n",
    "print(\"=== Testing Modern GeoscienceChatAgent ===\")\n",
    "\n",
    "# Test conversation\n",
    "questions = [\n",
    "    \"Hello! Can you explain what you specialize in?\",\n",
    "    \"What is the difference between conventional and unconventional reservoirs?\",\n",
    "    \"How do geophysicists use seismic data to find oil?\",\n",
    "    \"What role does well logging play in this process?\"\n",
    "]\n",
    "\n",
    "session_id = chat_agent.create_new_session()\n",
    "print(f\"Created session: {session_id[:8]}...\\n\")\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"{i}. Human: {question}\")\n",
    "    response = chat_agent.chat(question, session_id)\n",
    "    print(f\"   Dr. GeoBot: {response}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d8081",
   "metadata": {},
   "source": [
    "# 3. Building a Gradio UI\n",
    "\n",
    "Gradio is a popular open-source library to build web UIs for ML models with minimal code. The core idea is to define components (text boxes, buttons, chat windows) and wire them to Python functions via events. You can access the app in a browser and share it via a public link.\n",
    "\n",
    "Here is a link to the [Gradio documentation](https://gradio.app/docs/) for more details.\n",
    "\n",
    "Core components:\n",
    "- `gr.Blocks`: Page/layout container.\n",
    "- `gr.Chatbot`: Conversation pane (list of (user, bot) tuples).\n",
    "- `gr.Textbox`: Input field for user messages.\n",
    "- `gr.Button`: Send, Clear, Load examples.\n",
    "- Events: `.submit` and `.click` wire UI to Python callbacks.\n",
    "\n",
    "UX tips:\n",
    "- Keep answers concise; use memory to reduce repetition.\n",
    "- Provide example prompts to guide first-time users.\n",
    "- Add a “Clear chat” to reset session/memory.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Snippet: Minimal Gradio event handlers\n",
    "\n",
    "```python\n",
    "def respond(message, history):\n",
    "    reply = agent.chat(message, session_id)\n",
    "    history.append((message, reply))\n",
    "    return \"\", history\n",
    "\n",
    "msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "clear_btn.click(lambda: agent.clear_memory(session_id) or [], outputs=chatbot)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "# Create a new chat agent for the interface\n",
    "gradio_agent = ModernGeoscienceChatAgent(chat_model)\n",
    "\n",
    "# Global session management\n",
    "current_session = gradio_agent.create_new_session()\n",
    "\n",
    "def respond(message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Process user message and return bot response\n",
    "    \"\"\"\n",
    "    global current_session\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    # Get response from agent\n",
    "    bot_response = gradio_agent.chat(message, current_session)\n",
    "    \n",
    "    # Add to chat history\n",
    "    history.append((message, bot_response))\n",
    "    \n",
    "    return \"\", history\n",
    "\n",
    "def clear_conversation() -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Clear conversation history and start new session\n",
    "    \"\"\"\n",
    "    global current_session\n",
    "    gradio_agent.clear_memory(current_session)\n",
    "    current_session = gradio_agent.create_new_session()\n",
    "    return []\n",
    "\n",
    "def load_example(example: str) -> str:\n",
    "    \"\"\"\n",
    "    Load example question into the textbox\n",
    "    \"\"\"\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create modern Gradio interface\n",
    "with gr.Blocks(\n",
    "    title=\"Dr. GeoBot - Advanced Geoscience Chat Assistant\",\n",
    "    theme=gr.themes.Monochrome(\n",
    "        font=[gr.themes.GoogleFont(\"JetBrains Mono\"), \"monospace\"],\n",
    "    )\n",
    ") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🌍 Dr. GeoBot - Your Advanced Geoscience Expert\n",
    "    \n",
    "    I'm an AI geoscience expert powered by modern LangChain. Ask me about:\n",
    "    \n",
    "    | **Geophysics** | **Petroleum Engineering** | **Well Logging** |\n",
    "    |---|---|---|\n",
    "    | Seismic interpretation | Reservoir characterization | Formation evaluation |\n",
    "    | Gravity & magnetics | Hydrocarbon systems | Petrophysics |\n",
    "    | Electromagnetics | Production optimization | Log analysis |\n",
    "    \n",
    "    💡 *I remember our conversation, so feel free to ask follow-up questions!*\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                value=[],\n",
    "                height=500,\n",
    "                show_label=False,\n",
    "                bubble_full_width=False\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Ask me about geoscience topics...\",\n",
    "                    show_label=False,\n",
    "                    scale=4,\n",
    "                    container=False\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send 📤\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"🗑️ Clear Chat\", variant=\"secondary\")\n",
    "                \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### 💡 Example Questions\")\n",
    "            \n",
    "            example_questions = [\n",
    "                \"What is seismic inversion?\",\n",
    "                \"Explain porosity vs permeability\",\n",
    "                \"How do P-waves and S-waves differ?\",\n",
    "                \"What is reservoir characterization?\",\n",
    "                \"How does well logging work?\",\n",
    "                \"What are the challenges in unconventional reservoirs?\"\n",
    "            ]\n",
    "            \n",
    "            for question in example_questions:\n",
    "                example_btn = gr.Button(\n",
    "                    question,\n",
    "                    variant=\"secondary\",\n",
    "                    size=\"sm\"\n",
    "                )\n",
    "                example_btn.click(\n",
    "                    load_example,\n",
    "                    inputs=[gr.State(question)],\n",
    "                    outputs=msg\n",
    "                )\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear_btn.click(clear_conversation, outputs=chatbot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0adb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the interface\n",
    "print(\"Launching modern Gradio interface...\")\n",
    "demo.launch(share=True, show_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d093b",
   "metadata": {},
   "source": [
    "# 4. Troubleshooting and performance\n",
    "\n",
    "- Slow replies\n",
    "  - Lower max_new_tokens; use smaller model; close other notebooks\n",
    "- Repetition/rambling\n",
    "  - Lower temperature; add repetition_penalty=1.1; cap turn count\n",
    "- Tokenizer/pad errors\n",
    "  - tokenizer.pad_token = tokenizer.eos_token\n",
    "- BitsAndBytes import errors (Mac)\n",
    "  - Skip 4-bit; load small model with torch_dtype=torch.float16 on MPS\n",
    "- Memory not “sticking”\n",
    "  - Ensure you pass config={\"configurable\": {\"session_id\": ...}} on each call"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
