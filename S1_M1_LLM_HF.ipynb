{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4820df32",
   "metadata": {},
   "source": [
    "![Workshop Banner](assets/S1_M1.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S1_M1_LLM_HF.ipynb)\n",
    "\n",
    "***\n",
    "# Session 01 // Module 01: Large Language Models (LLMs) with HuggingFace\n",
    "\n",
    "In this module, we'll explore the fundamentals of Large Language Models (LLMs) using HuggingFace Transformers. We'll cover tokens, embeddings, context windows, and hands-on text generation with a focus on geoscience applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand tokens, subword tokenization, embeddings, and context windows\n",
    "- Load and use a small HuggingFace model for inference\n",
    "- Generate text with controlled decoding parameters\n",
    "- Visualize and interpret embeddings for geoscience texts\n",
    "- Apply LLMs to simple geoscience definition tasks\n",
    "\n",
    "## What you’ll build\n",
    "- A minimal pipeline to tokenize text, produce embeddings, and visualize them\n",
    "- A lightweight text generation setup using a small causal LM\n",
    "- Exercises to craft prompts and compare decoding strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ef834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/SPE_GeoHackathon_2025/refs/heads/dev/spe_utils.txt -O spe_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=5 -i spe_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# Retrieving the token is required to get access to HF hub\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spe_utils.data import (\n",
    "    GEOSCIENCE_TERMS,\n",
    "    TOKENIZATION_EXAMPLES,\n",
    "    GEOPHYSICS_TEXTS,\n",
    "    GEOPHYSICS_CATEGORIES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "#VSC-99db4dcf",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Understanding Tokens\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: **Tokens** are the basic units LLMs process. Modern tokenizers use subword schemes (WordPiece, BPE) to split words into frequently occurring chunks.\n",
    "\n",
    "A token can be:\n",
    "- a whole word (e.g., \"seismic\")\n",
    "- a subword (e.g., \"sei\", \"##smic\")\n",
    "- punctuation or special symbols\n",
    "\n",
    "Why subwords?\n",
    "- Handle rare words and morphology better\n",
    "- Keep the vocabulary compact while covering most text\n",
    "\n",
    "Example (WordPiece/BPE style):\n",
    "\n",
    "```text\n",
    "reservoir → [\"reservoir\"]\n",
    "microfracture → [\"micro\", \"##fract\", \"##ure\"]\n",
    "wellbore-stability → [\"well\", \"##bore\", \"-\", \"stability\"]\n",
    "```\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Tip**: Different models use different tokenizers (BERT uses WordPiece; GPT-2 uses BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d663928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from spe_utils.visualisation import bert_tokenize_and_color\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df60458",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in TOKENIZATION_EXAMPLES:\n",
    "    bert_tokenize_and_color(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample vocabulary, special tokens, and token mapping\n",
    "\n",
    "# Sample vocab (first 20 keys)\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"Sample vocabulary (first 20):\", list(vocab.keys())[:20])\n",
    "\n",
    "# Special tokens\n",
    "print(\"\\nSpecial tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# Mapping for the first tokenization example\n",
    "sample_text = TOKENIZATION_EXAMPLES[0]\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"\\nSample text: {sample_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Full encoding for the first example\n",
    "encoded = tokenizer(sample_text, return_tensors='pt')\n",
    "print(f\"\\nFull encoding (input_ids): {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded tokens: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "#VSC-d34ec505",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Understanding Embeddings\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: **Embeddings** map tokens or sentences to dense vectors that capture semantic relationships. Nearby vectors are semantically similar.\n",
    "\n",
    "Key concepts:\n",
    "- Static vs contextual:\n",
    "  - Static (e.g., GloVe) assigns one vector per word\n",
    "  - Contextual (e.g., BERT, MiniLM) depends on surrounding words\n",
    "- Dimensions: Common sizes are 384, 512, 768, 1024+\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Evaluation tip**: Use cosine similarity to compare embeddings. Values closer to 1.0 indicate higher similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a small model for embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b58f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"Get sentence embeddings\"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use CLS token embedding (first token) for sentence representation\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Get embeddings for geoscience terms\n",
    "geoscience_terms = [\n",
    "    \"seismic inversion\",\n",
    "    \"reservoir characterization\", \n",
    "    \"hydrocarbon exploration\",\n",
    "    \"petrophysical analysis\",\n",
    "    \"porosity measurement\",\n",
    "    \"permeability analysis\"\n",
    "]\n",
    "\n",
    "# Get embeddings for geoscience terms\n",
    "# Remove the hardcoded list and use the imported constant\n",
    "embeddings = get_embeddings(GEOSCIENCE_TERMS, tokenizer, model)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each term is represented by {embeddings.shape[1]} numbers\")\n",
    "print(f\"\\nFirst 10 embedding values for '{GEOSCIENCE_TERMS[0]}':\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of geophysics texts: {len(GEOPHYSICS_TEXTS)}\")\n",
    "print(\"Sample texts:\")\n",
    "for i, text in enumerate(GEOPHYSICS_TEXTS[:5]):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b87e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Encode all geophysics sentences\n",
    "inputs = tokenizer(GEOPHYSICS_TEXTS, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).last_hidden_state[:,0,:]  # CLS token\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented by {embeddings.shape[1]} dimensional vector\")\n",
    "\n",
    "# Reduce dimensions to 3D with t-SNE\n",
    "perplexity = min(30, len(GEOPHYSICS_TEXTS) - 1)\n",
    "tsne = TSNE(n_components=3, perplexity=perplexity, random_state=42, max_iter=1000)\n",
    "embeddings_3d = tsne.fit_transform(embeddings.numpy())\n",
    "\n",
    "print(f\"3D embeddings shape: {embeddings_3d.shape}\")\n",
    "print(f\"Using perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaad5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Create the 3D scatter plot using imported data\n",
    "fig = px.scatter_3d(\n",
    "    x=embeddings_3d[:,0],\n",
    "    y=embeddings_3d[:,1],\n",
    "    z=embeddings_3d[:,2],\n",
    "    hover_name=GEOPHYSICS_TEXTS,  # Use imported data\n",
    "    color=GEOPHYSICS_CATEGORIES,  # Use imported categories\n",
    "    title=\"Interactive 3D Geophysics Text Embeddings\",\n",
    "    labels={'x':'Dimension 1', 'y':'Dimension 2', 'z':'Dimension 3'},\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "fig.update_layout(\n",
    "    template='plotly_dark', font_family='monospace', width=900, height=700)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b402491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze semantic similarities within categories\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings.numpy())\n",
    "\n",
    "# Find most similar sentence pairs\n",
    "similarity_pairs = []\n",
    "for i in range(len(GEOPHYSICS_TEXTS)):\n",
    "    for j in range(i+1, len(GEOPHYSICS_TEXTS)):\n",
    "        similarity_pairs.append({\n",
    "            'text1': GEOPHYSICS_TEXTS[i][:50] + '...',\n",
    "            'text2': GEOPHYSICS_TEXTS[j][:50] + '...',\n",
    "            'category1': GEOPHYSICS_CATEGORIES[i],\n",
    "            'category2': GEOPHYSICS_CATEGORIES[j],\n",
    "            'similarity': similarity_matrix[i, j],\n",
    "            'same_category': GEOPHYSICS_CATEGORIES[i] == GEOPHYSICS_CATEGORIES[j]\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and sort by similarity\n",
    "df_similarities = pd.DataFrame(similarity_pairs)\n",
    "df_top_similar = df_similarities.nlargest(10, 'similarity')\n",
    "\n",
    "print(\"Top 10 Most Similar Sentence Pairs:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in df_top_similar.iterrows():\n",
    "    same_cat = \"✓\" if row['same_category'] else \"✗\"\n",
    "    print(f\"Similarity: {row['similarity']:.3f} | Same Category: {same_cat}\")\n",
    "    print(f\"1. [{row['category1']}] {row['text1']}\")\n",
    "    print(f\"2. [{row['category2']}] {row['text2']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Calculate average similarity within vs between categories\n",
    "within_category_sim = df_similarities[df_similarities['same_category']]['similarity'].mean()\n",
    "between_category_sim = df_similarities[~df_similarities['same_category']]['similarity'].mean()\n",
    "\n",
    "print(f\"\\nAverage similarity within same category: {within_category_sim:.3f}\")\n",
    "print(f\"Average similarity between different categories: {between_category_sim:.3f}\")\n",
    "print(f\"Difference: {within_category_sim - between_category_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "#VSC-922f4326",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Understanding Context Windows\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**:The **context window** is the maximum number of tokens a model processes at once. Both your prompt and generated continuation must fit within this limit.\n",
    "\n",
    "Why it matters:\n",
    "- Limits how much the model “remembers” at inference time\n",
    "- Affects truncation and chunking strategies for long documents\n",
    "- Impacts latency and memory usage\n",
    "\n",
    "Typical sizes (approximate):\n",
    "- GPT-2 family: 1,024–2,048 tokens\n",
    "- Modern LLMs: 4k–200k+ tokens, depending on the model\n",
    "\n",
    "Key parameters to watch:\n",
    "- `tokenizer.model_max_length` or `config.n_positions`\n",
    "- `max_new_tokens` vs `max_length` (prefer `max_new_tokens` to avoid counting prompt tokens implicitly)\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Chunking long text (sliding window) example:\n",
    "\n",
    "```python\n",
    "def chunk_tokens(tokenizer, text, max_len=1024, overlap=50):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(ids):\n",
    "        end = min(start + max_len, len(ids))\n",
    "        chunks.append(ids[start:end])\n",
    "        if end == len(ids):\n",
    "            break\n",
    "        start = end - overlap  # slide back by overlap\n",
    "    return chunks\n",
    "\n",
    "# Each chunk can be fed independently; aggregate results later.\n",
    "```\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Tip**: Leave a margin in the window for generation. Example: if the model limit is 1024, keep the prompt ≤ 900 tokens and use `max_new_tokens` ≤ 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context window limitations\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token\n",
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
    "\n",
    "print(f\"GPT-2 maximum position embeddings: {model_gpt2.config.n_positions}\")\n",
    "print(f\"This means the context window is {model_gpt2.config.n_positions} tokens\")\n",
    "\n",
    "# Create a long geoscience text to test context limits\n",
    "long_text = \"\"\"\n",
    "Seismic inversion is a geophysical technique used to derive subsurface properties from seismic data. \n",
    "The process involves converting seismic reflection data into quantitative rock and fluid properties such as \n",
    "acoustic impedance, porosity, and lithology. This technique is fundamental in hydrocarbon exploration \n",
    "and reservoir characterization. The inversion process typically starts with seismic data acquisition, \n",
    "followed by data processing, and finally the inversion itself. There are several types of seismic inversion \n",
    "including post-stack inversion, pre-stack inversion, and simultaneous inversion. Post-stack inversion \n",
    "works with stacked seismic data to derive acoustic impedance. Pre-stack inversion uses angle-dependent \n",
    "reflectivity information to derive multiple elastic properties. Simultaneous inversion integrates seismic \n",
    "and well log data to provide more accurate and detailed subsurface models.\n",
    "\"\"\" * 10  # Repeat to make it longer\n",
    "\n",
    "# Tokenize the long text\n",
    "tokens = tokenizer_gpt2.tokenize(long_text)\n",
    "print(f\"\\nLong text has {len(tokens)} tokens\")\n",
    "print(f\"Exceeds context window: {len(tokens) > model_gpt2.config.n_positions}\")\n",
    "\n",
    "# Show what happens when we truncate\n",
    "max_length = model_gpt2.config.n_positions - 50  # Leave room for generation\n",
    "truncated_tokens = tokens[:max_length]\n",
    "print(f\"Truncated to {len(truncated_tokens)} tokens for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "#VSC-de7e04ce",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Loading a Small HuggingFace Model\n",
    "\n",
    "We’ll use a compact causal language model for fast experimentation. Smaller models are great for demos and offline inference but will have limited knowledge and coherence compared to larger models.\n",
    "\n",
    "Considerations:\n",
    "- Trade-offs: size vs speed vs quality\n",
    "- Device placement: CPU, GPU, or Apple MPS\n",
    "- Precision: fp32 (safe), fp16/bf16 (faster on supported hardware)\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Convenient loading patterns:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = 'distilgpt2'\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(model_name)\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Device selection\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_gen = model_gen.to(device)\n",
    "\n",
    "# Pad/eos safety for generation\n",
    "if tokenizer_gen.pad_token_id is None:\n",
    "    tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
    "```\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Tip (advanced)**: For larger models that fit in memory, try `device_map='auto'` and `torch_dtype=torch.float16` on supported hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2976d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load a small, efficient model for text generation\n",
    "model_name = \"distilgpt2\"  # Smaller, faster version of GPT-2\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(model_name)\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token\n",
    "if tokenizer_gen.pad_token is None:\n",
    "    tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer_gen.vocab_size:,}\")\n",
    "print(f\"Model parameters: {model_gen.num_parameters():,}\")\n",
    "print(f\"Context window: {model_gen.config.n_positions} tokens\")\n",
    "print(f\"Embedding dimension: {model_gen.config.n_embd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "#VSC-614a6ce9",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 5. Generate Simple Text Completions\n",
    "\n",
    "We’ll generate short continuations and compare decoding strategies.\n",
    "\n",
    "Decoding parameters (cheat sheet):\n",
    "- `max_new_tokens`: number of tokens to generate (prefer over `max_length`)\n",
    "- `temperature`: randomness (↓ = conservative, ↑ = creative)\n",
    "- `top_k`: Only consider the top K most likely next tokens. Smaller K = safer, larger K = more variety.\n",
    "- `top_p` (nucleus): Only consider the smallest set of tokens whose total probability ≥ p (e.g., 0.9). Lower p = safer.\n",
    "- `repetition_penalty`: Punishes repeating the same text. Use >1.0 (e.g., 1.1–1.3) to reduce loops.\n",
    "- `no_repeat_ngram_size`: Forbids repeating any exact phrase of length n (e.g., 2 prevents repeating bigrams).\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Example generation call:\n",
    "\n",
    "```python\n",
    "prompt = 'Reservoir characterization involves'\n",
    "inputs = tokenizer_gen(prompt, return_tensors='pt').to(model_gen.device)\n",
    "with torch.no_grad():\n",
    "    out_ids = model_gen.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer_gen.eos_token_id,\n",
    "        pad_token_id=tokenizer_gen.pad_token_id,\n",
    "    )\n",
    "print(tokenizer_gen.decode(out_ids[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Tips**\n",
    "> - Keep prompts concise and specific. Prefix with context if needed (e.g., “Geoscience definition: …”).\n",
    "> - Use either top_k or top_p (top_p is usually easier to tune).\n",
    "> - These work only when do_sample=True (otherwise decoding is greedy).\n",
    "> - Prefer max_new_tokens over max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45267a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, tokenizer, model, max_length=100, temperature=0.7, num_return_sequences=1):\n",
    "    \"\"\"Generate text completion given a prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2  # Avoid repetition\n",
    "        )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Test with simple prompts\n",
    "simple_prompts = [\n",
    "    \"The geology of this region\",\n",
    "    \"Oil and gas exploration requires\",\n",
    "    \"Seismic waves travel through\"\n",
    "]\n",
    "\n",
    "print(\"=== Simple Text Completions ===\")\n",
    "for prompt in simple_prompts:  # Use imported prompts\n",
    "    generated = generate_text(prompt, tokenizer_gen, model_gen, max_length=60)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Completion: '{generated[0]}'\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad93d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different generation parameters\n",
    "prompt = \"Reservoir characterization involves\"\n",
    "\n",
    "print(\"=== Effect of Different Parameters ===\")\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "low_temp = generate_text(prompt, tokenizer_gen, model_gen, max_length=50, temperature=0.3)\n",
    "print(f\"Low temperature (0.3): {low_temp[0]}\")\n",
    "\n",
    "# High temperature (more creative)\n",
    "high_temp = generate_text(prompt, tokenizer_gen, model_gen, max_length=50, temperature=1.2)\n",
    "print(f\"High temperature (1.2): {high_temp[0]}\")\n",
    "\n",
    "# Multiple generations\n",
    "multiple = generate_text(prompt, tokenizer_gen, model_gen, max_length=50, temperature=0.8, num_return_sequences=3)\n",
    "print(\"\\nMultiple generations:\")\n",
    "for i, gen in enumerate(multiple, 1):\n",
    "    print(f\"{i}. {gen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cd37f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we covered:\n",
    "\n",
    "1. **Tokens**: Basic units that LLMs process (words, subwords, punctuation)\n",
    "2. **Embeddings**: Numerical representations that capture semantic meaning\n",
    "3. **Context Windows**: Maximum input size limitations (1,024 tokens for GPT-2)\n",
    "4. **Model Loading**: Using HuggingFace transformers to load pre-trained models\n",
    "5. **Text Generation**: Creating completions with different parameters\n",
    "6. **Geoscience Applications**: Generating definitions for technical terms\n",
    "\n",
    "### Key Takeaways:\n",
    "- Tokenization breaks text into processable units\n",
    "- Embeddings capture semantic relationships between concepts\n",
    "- Context windows limit how much text models can process at once\n",
    "- Different prompting strategies can yield different results\n",
    "- Temperature controls randomness in generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "#VSC-cfe4f155",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 6. Exercise: Geoscience Definition Generation\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Task**: Generate concise, factual definitions for geoscience terms.\n",
    "\n",
    "Guidelines:\n",
    "- Aim for 1–3 sentences per definition\n",
    "- Prefer domain-appropriate vocabulary\n",
    "- Keep statements verifiable and neutral\n",
    "\n",
    "Suggested prompt templates:\n",
    "\n",
    "```text\n",
    "Define \"{term}\" in geoscience.\n",
    "In petroleum geoscience, what is {term}?\n",
    "Give a brief, technical definition of {term}.\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> **Snippet**: Example helper (pseudocode you can adapt in the code cell):\n",
    "\n",
    "```python\n",
    "def generate_definition(term, tokenizer, model, max_new_tokens=80, temperature=0.6):\n",
    "    prompt = f\"Geoscience definition: {term}. Definition:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "Advanced challenge:\n",
    "- Compare three prompts for the same term and evaluate which is clearer\n",
    "- Embed each generated definition and compute cosine similarity between them\n",
    "- Visualize definitions in 2D (t-SNE/UMAP) to see clustering by prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exercise: Generate geoscience definitions\n",
    "# # STEP 1: Define a method to generate definitions using\n",
    "# # a predifined prompt\n",
    "# def generate_definition(term, tokenizer, model, max_length=150):\n",
    "#     \"\"\"Generate a definition for a geoscience term\"\"\"\n",
    "\n",
    "#     \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "#     prompt = \n",
    "    \n",
    "#     return generated[0]\n",
    "\n",
    "# # Main exercise: Seismic inversion\n",
    "# print(\"=== MAIN EXERCISE: Seismic Inversion Definition ===\")\n",
    "# seismic_inversion_def = generate_definition(\"seismic inversion\", tokenizer_gen, model_gen)\n",
    "# print(seismic_inversion_def)\n",
    "# print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# # Additional geoscience terms to try\n",
    "# geoscience_terms_exercise = [\n",
    "#     \"porosity\",\n",
    "#     \"permeability\", \n",
    "#     \"reservoir characterization\",\n",
    "#     \"hydrocarbon migration\",\n",
    "#     \"seismic interpretation\",\n",
    "#     \"well logging\"\n",
    "# ]\n",
    "\n",
    "# print(\"=== Additional Geoscience Definitions ===\")\n",
    "# # STEP 2: Iterate over the list and generate definitions\n",
    "# \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8722d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Advanced exercise: Compare different prompting strategies\n",
    "# term = \"seismic inversion\"\n",
    "\n",
    "# # STEP 3: Create a list of different prompting strategies\n",
    "# # These strategies will be used to generate definitions for the term\n",
    "# \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "# prompting_strategies = \n",
    "\n",
    "# # STEP 4: Iterate through strategies and generate definitions\n",
    "# print(\"=== Comparing Prompting Strategies ===\")\n",
    "# for i, prompt in enumerate(prompting_strategies, 1):\n",
    "\n",
    "#     \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "#     print(f\"\\nStrategy {i}: '{prompt}'\")\n",
    "#     print(f\"Response: {generated[0]}\")\n",
    "#     print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
