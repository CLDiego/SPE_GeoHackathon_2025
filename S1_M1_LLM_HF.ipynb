{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4820df32",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/dev/S1_M1_LLM_HF.ipynb)\n",
    "\n",
    "***\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Follow along by running each cell in order\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Make sure to run the environment setup cells first\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Wait for each installation to complete before proceeding\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if installations take a while - this is normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ef834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/SPE_GeoHackathon_2025/refs/heads/dev/spe_utils.txt -O spe_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=5 -i spe_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup [If running outside Colab]\n",
    "# !pip install transformers torch matplotlib plotly scikit-learn ipython\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face API token\n",
    "# # Retrieving the token is required to get access to HF hub\n",
    "# from google.colab import userdata\n",
    "# hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spe_utils.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spe_utils.data import (\n",
    "    GEOSCIENCE_TERMS,\n",
    "    TOKENIZATION_EXAMPLES,\n",
    "    SIMPLE_PROMPTS,\n",
    "    GEOPHYSICS_TEXTS,\n",
    "    GEOPHYSICS_CATEGORIES,\n",
    "    get_texts_by_category,\n",
    "    get_available_categories,\n",
    "    get_random_texts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7ecac",
   "metadata": {},
   "source": [
    "# Session 01 // Module 01: Large Language Models (LLMs) with HuggingFace\n",
    "\n",
    "In this module, we'll explore the fundamentals of Large Language Models (LLMs) using HuggingFace transformers. We'll cover tokens, embeddings, context windows, and hands-on text generation with a focus on geoscience applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what tokens, embeddings, and context windows are\n",
    "- Load and use a small HuggingFace model\n",
    "- Generate simple text completions\n",
    "- Apply LLMs to geoscience definition tasks\n",
    "\n",
    "## 1. Understanding Tokens\n",
    "\n",
    "**Tokens** are the basic units that language models work with. Text is broken down into tokens before being processed by the model. A token can be:\n",
    "- A whole word (e.g., \"seismic\")\n",
    "- Part of a word (e.g., \"seis\", \"mic\")\n",
    "- Punctuation marks\n",
    "- Special symbols\n",
    "\n",
    "Let's see how tokenization works with a geoscience example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d663928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from spe_utils.visualisation import bert_tokenize_and_color\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df60458",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in TOKENIZATION_EXAMPLES:\n",
    "    bert_tokenize_and_color(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample vocabulary, special tokens, and token mapping\n",
    "\n",
    "# Sample vocab (first 20 keys)\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"Sample vocabulary (first 20):\", list(vocab.keys())[:20])\n",
    "\n",
    "# Special tokens\n",
    "print(\"\\nSpecial tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# Mapping for the first tokenization example\n",
    "sample_text = TOKENIZATION_EXAMPLES[0]\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"\\nSample text: {sample_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Full encoding for the first example\n",
    "encoded = tokenizer(sample_text, return_tensors='pt')\n",
    "print(f\"\\nFull encoding (input_ids): {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded tokens: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ecf0c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- You can use `AutoTokenizer` for automatic model selection.\n",
    "- To perform tokenization, you can use the `tokenizer` object created from the `BertTokenizer` class or the `AutoTokenizer` class.\n",
    "\n",
    "## 2. Understanding Embeddings\n",
    "\n",
    "**Embeddings** are numerical representations of tokens in a high-dimensional space. Each token is converted to a vector of numbers that captures its meaning and relationships to other tokens.\n",
    "\n",
    "Key properties of embeddings:\n",
    "- Similar words have similar embeddings\n",
    "- Embeddings capture semantic relationships\n",
    "- Typical dimensions: 512, 768, 1024, or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a small model for embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer_embed = AutoTokenizer.from_pretrained(model_name)\n",
    "model_embed = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b58f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"Get sentence embeddings\"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use CLS token embedding (first token) for sentence representation\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Get embeddings for geoscience terms\n",
    "geoscience_terms = [\n",
    "    \"seismic inversion\",\n",
    "    \"reservoir characterization\", \n",
    "    \"hydrocarbon exploration\",\n",
    "    \"petrophysical analysis\",\n",
    "    \"porosity measurement\",\n",
    "    \"permeability analysis\"\n",
    "]\n",
    "\n",
    "# Get embeddings for geoscience terms\n",
    "# Remove the hardcoded list and use the imported constant\n",
    "embeddings = get_embeddings(GEOSCIENCE_TERMS, tokenizer_embed, model_embed)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each term is represented by {embeddings.shape[1]} numbers\")\n",
    "print(f\"\\nFirst 10 embedding values for '{GEOSCIENCE_TERMS[0]}':\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model + tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Remove the entire hardcoded geophysics_texts list and replace with:\n",
    "print(f\"Total number of geophysics texts: {len(GEOPHYSICS_TEXTS)}\")\n",
    "print(\"Sample texts:\")\n",
    "for i, text in enumerate(GEOPHYSICS_TEXTS[:5]):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b87e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all geophysics sentences\n",
    "inputs = tokenizer(GEOPHYSICS_TEXTS, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).last_hidden_state[:,0,:]  # CLS token\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented by {embeddings.shape[1]} dimensional vector\")\n",
    "\n",
    "# Reduce dimensions to 3D with t-SNE\n",
    "perplexity = min(30, len(GEOPHYSICS_TEXTS) - 1)\n",
    "tsne = TSNE(n_components=3, perplexity=perplexity, random_state=42, max_iter=1000)\n",
    "embeddings_3d = tsne.fit_transform(embeddings.numpy())\n",
    "\n",
    "print(f\"3D embeddings shape: {embeddings_3d.shape}\")\n",
    "print(f\"Using perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaad5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Create the 3D scatter plot using imported data\n",
    "fig = px.scatter_3d(\n",
    "    x=embeddings_3d[:,0],\n",
    "    y=embeddings_3d[:,1],\n",
    "    z=embeddings_3d[:,2],\n",
    "    hover_name=GEOPHYSICS_TEXTS,  # Use imported data\n",
    "    color=GEOPHYSICS_CATEGORIES,  # Use imported categories\n",
    "    title=\"Interactive 3D Geophysics Text Embeddings\",\n",
    "    labels={'x':'Dimension 1', 'y':'Dimension 2', 'z':'Dimension 3'},\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "fig.update_layout(\n",
    "    template='plotly_dark', font_family='monospace', width=900, height=700)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b402491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze semantic similarities within categories\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings.numpy())\n",
    "\n",
    "# Find most similar sentence pairs\n",
    "similarity_pairs = []\n",
    "for i in range(len(GEOPHYSICS_TEXTS)):\n",
    "    for j in range(i+1, len(GEOPHYSICS_TEXTS)):\n",
    "        similarity_pairs.append({\n",
    "            'text1': GEOPHYSICS_TEXTS[i][:50] + '...',\n",
    "            'text2': GEOPHYSICS_TEXTS[j][:50] + '...',\n",
    "            'category1': GEOPHYSICS_CATEGORIES[i],\n",
    "            'category2': GEOPHYSICS_CATEGORIES[j],\n",
    "            'similarity': similarity_matrix[i, j],\n",
    "            'same_category': GEOPHYSICS_CATEGORIES[i] == GEOPHYSICS_CATEGORIES[j]\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and sort by similarity\n",
    "df_similarities = pd.DataFrame(similarity_pairs)\n",
    "df_top_similar = df_similarities.nlargest(10, 'similarity')\n",
    "\n",
    "print(\"Top 10 Most Similar Sentence Pairs:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in df_top_similar.iterrows():\n",
    "    same_cat = \"✓\" if row['same_category'] else \"✗\"\n",
    "    print(f\"Similarity: {row['similarity']:.3f} | Same Category: {same_cat}\")\n",
    "    print(f\"1. [{row['category1']}] {row['text1']}\")\n",
    "    print(f\"2. [{row['category2']}] {row['text2']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Calculate average similarity within vs between categories\n",
    "within_category_sim = df_similarities[df_similarities['same_category']]['similarity'].mean()\n",
    "between_category_sim = df_similarities[~df_similarities['same_category']]['similarity'].mean()\n",
    "\n",
    "print(f\"\\nAverage similarity within same category: {within_category_sim:.3f}\")\n",
    "print(f\"Average similarity between different categories: {between_category_sim:.3f}\")\n",
    "print(f\"Difference: {within_category_sim - between_category_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56368ac5",
   "metadata": {},
   "source": [
    "## 3. Understanding Context Windows\n",
    "\n",
    "**Context window** refers to the maximum number of tokens a model can process at once. This is a crucial limitation that affects:\n",
    "- How much text the model can \"remember\"\n",
    "- The maximum input size for generation tasks\n",
    "- Computational requirements\n",
    "\n",
    "Common context window sizes:\n",
    "- GPT-2: 1,024 tokens\n",
    "- GPT-3: 4,096 tokens  \n",
    "- GPT-4: 8,192 - 32,768 tokens\n",
    "- Claude: 100,000+ tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context window limitations\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token\n",
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
    "\n",
    "print(f\"GPT-2 maximum position embeddings: {model_gpt2.config.n_positions}\")\n",
    "print(f\"This means the context window is {model_gpt2.config.n_positions} tokens\")\n",
    "\n",
    "# Create a long geoscience text to test context limits\n",
    "long_text = \"\"\"\n",
    "Seismic inversion is a geophysical technique used to derive subsurface properties from seismic data. \n",
    "The process involves converting seismic reflection data into quantitative rock and fluid properties such as \n",
    "acoustic impedance, porosity, and lithology. This technique is fundamental in hydrocarbon exploration \n",
    "and reservoir characterization. The inversion process typically starts with seismic data acquisition, \n",
    "followed by data processing, and finally the inversion itself. There are several types of seismic inversion \n",
    "including post-stack inversion, pre-stack inversion, and simultaneous inversion. Post-stack inversion \n",
    "works with stacked seismic data to derive acoustic impedance. Pre-stack inversion uses angle-dependent \n",
    "reflectivity information to derive multiple elastic properties. Simultaneous inversion integrates seismic \n",
    "and well log data to provide more accurate and detailed subsurface models.\n",
    "\"\"\" * 10  # Repeat to make it longer\n",
    "\n",
    "# Tokenize the long text\n",
    "tokens = tokenizer_gpt2.tokenize(long_text)\n",
    "print(f\"\\nLong text has {len(tokens)} tokens\")\n",
    "print(f\"Exceeds context window: {len(tokens) > model_gpt2.config.n_positions}\")\n",
    "\n",
    "# Show what happens when we truncate\n",
    "max_length = model_gpt2.config.n_positions - 50  # Leave room for generation\n",
    "truncated_tokens = tokens[:max_length]\n",
    "print(f\"Truncated to {len(truncated_tokens)} tokens for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e6952",
   "metadata": {},
   "source": [
    "## 4. Loading a Small HuggingFace Model\n",
    "\n",
    "Let's load and explore a small language model suitable for text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2976d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a small, efficient model for text generation\n",
    "model_name = \"distilgpt2\"  # Smaller, faster version of GPT-2\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(model_name)\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token\n",
    "if tokenizer_gen.pad_token is None:\n",
    "    tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer_gen.vocab_size:,}\")\n",
    "print(f\"Model parameters: {model_gen.num_parameters():,}\")\n",
    "print(f\"Context window: {model_gen.config.n_positions} tokens\")\n",
    "print(f\"Embedding dimension: {model_gen.config.n_embd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664990a",
   "metadata": {},
   "source": [
    "## 5. Generate Simple Text Completions\n",
    "\n",
    "Now let's use our model to generate text completions with various prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45267a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, tokenizer, model, max_length=100, temperature=0.7, num_return_sequences=1):\n",
    "    \"\"\"Generate text completion given a prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2  # Avoid repetition\n",
    "        )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Test with simple prompts\n",
    "simple_prompts = [\n",
    "    \"The geology of this region\",\n",
    "    \"Oil and gas exploration requires\",\n",
    "    \"Seismic waves travel through\"\n",
    "]\n",
    "\n",
    "print(\"=== Simple Text Completions ===\")\n",
    "for prompt in simple_prompts:  # Use imported prompts\n",
    "    generated = generate_text(prompt, tokenizer_gen, model_gen, max_length=60)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Completion: '{generated[0]}'\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad93d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different generation parameters\n",
    "prompt = \"Reservoir characterization involves\"\n",
    "\n",
    "print(\"=== Effect of Different Parameters ===\")\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "low_temp = generate_text(prompt, tokenizer_gen, model_gen, max_length=50, temperature=0.3)\n",
    "print(f\"Low temperature (0.3): {low_temp[0]}\")\n",
    "\n",
    "# High temperature (more creative)\n",
    "high_temp = generate_text(prompt, tokenizer_gen, model_gen, max_length=50, temperature=1.2)\n",
    "print(f\"High temperature (1.2): {high_temp[0]}\")\n",
    "\n",
    "# Multiple generations\n",
    "multiple = generate_text(prompt, tokenizer_gen, model_gen, max_length=50, temperature=0.8, num_return_sequences=3)\n",
    "print(\"\\nMultiple generations:\")\n",
    "for i, gen in enumerate(multiple, 1):\n",
    "    print(f\"{i}. {gen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cd37f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we covered:\n",
    "\n",
    "1. **Tokens**: Basic units that LLMs process (words, subwords, punctuation)\n",
    "2. **Embeddings**: Numerical representations that capture semantic meaning\n",
    "3. **Context Windows**: Maximum input size limitations (1,024 tokens for GPT-2)\n",
    "4. **Model Loading**: Using HuggingFace transformers to load pre-trained models\n",
    "5. **Text Generation**: Creating completions with different parameters\n",
    "6. **Geoscience Applications**: Generating definitions for technical terms\n",
    "\n",
    "### Key Takeaways:\n",
    "- Tokenization breaks text into processable units\n",
    "- Embeddings capture semantic relationships between concepts\n",
    "- Context windows limit how much text models can process at once\n",
    "- Different prompting strategies can yield different results\n",
    "- Temperature controls randomness in generation\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with larger models for better geoscience definitions\n",
    "- Try fine-tuning models on domain-specific geoscience text\n",
    "- Explore retrieval-augmented generation (RAG) for factual accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380ba0f",
   "metadata": {},
   "source": [
    "## 6. Exercise: Geoscience Definition Generation\n",
    "\n",
    "**Your Task**: Use the model to generate definitions for various geoscience terms. We'll focus on \"What is seismic inversion?\" and other related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Generate geoscience definitions\n",
    "def generate_definition(term, tokenizer, model, max_length=150):\n",
    "    \"\"\"Generate a definition for a geoscience term\"\"\"\n",
    "    prompt = f\"What is {term}? {term} is\"\n",
    "    \n",
    "    generated = generate_text(\n",
    "        prompt, \n",
    "        tokenizer, \n",
    "        model, \n",
    "        max_length=max_length, \n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return generated[0]\n",
    "\n",
    "# Main exercise: Seismic inversion\n",
    "print(\"=== MAIN EXERCISE: Seismic Inversion Definition ===\")\n",
    "seismic_inversion_def = generate_definition(\"seismic inversion\", tokenizer_gen, model_gen)\n",
    "print(seismic_inversion_def)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Additional geoscience terms to try\n",
    "geoscience_terms_exercise = [\n",
    "    \"porosity\",\n",
    "    \"permeability\", \n",
    "    \"reservoir characterization\",\n",
    "    \"hydrocarbon migration\",\n",
    "    \"seismic interpretation\",\n",
    "    \"well logging\"\n",
    "]\n",
    "\n",
    "print(\"=== Additional Geoscience Definitions ===\")\n",
    "for term in geoscience_terms_exercise:\n",
    "    definition = generate_definition(term, tokenizer_gen, model_gen, max_length=100)\n",
    "    print(f\"\\n{term.upper()}:\")\n",
    "    print(definition)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8722d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced exercise: Compare different prompting strategies\n",
    "term = \"seismic inversion\"\n",
    "\n",
    "prompting_strategies = [\n",
    "    f\"What is {term}?\",\n",
    "    f\"Define {term}:\",\n",
    "    f\"{term} is a geophysical technique that\",\n",
    "    f\"In geophysics, {term} refers to\",\n",
    "    f\"Question: What is {term}?\\nAnswer:\"\n",
    "]\n",
    "\n",
    "print(\"=== Comparing Prompting Strategies ===\")\n",
    "for i, prompt in enumerate(prompting_strategies, 1):\n",
    "    generated = generate_text(prompt, tokenizer_gen, model_gen, max_length=80, temperature=0.6)\n",
    "    print(f\"\\nStrategy {i}: '{prompt}'\")\n",
    "    print(f\"Response: {generated[0]}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
